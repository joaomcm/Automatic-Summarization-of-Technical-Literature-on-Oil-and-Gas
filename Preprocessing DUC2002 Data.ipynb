{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating TS Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk.data\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from joblib import Parallel,delayed\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts_matrix(f,filename):\n",
    "    \"\"\"This function is responsible for getting the ts matrix from a given file in the DUC2002 format\n",
    "        inputs: f -> A raw open file in the DUC2002 format\n",
    "        filename --> The path to the file so we can get the name of the resulting documents\n",
    "        outputs: TS-matrix for this file, removing stopwords and with an added column \n",
    "        containing the filename for use in multiple document summarization\"\"\"\n",
    "    a = BeautifulSoup(f,'html.parser')\n",
    "    text = a.find('text').text.replace('\\n',' ')\n",
    "    this_file = filename.split(\"/\")[-1]\n",
    "    #we then proceed to preprocess the text:\n",
    "    #first by tokeninzing it\n",
    "    #detecting all the sentences:\n",
    "    sentences = sent_detector.tokenize(text)\n",
    "    #tokeninzing each sentence\n",
    "\n",
    "    tokenized = []\n",
    "    useful_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        if(any(c.isalpha() for c in sentences[i])):\n",
    "            tokenized.append(WordPunctTokenizer().tokenize(sentences[i].strip()))\n",
    "            useful_sentences.append(sentences[i].strip())\n",
    "    sentences = useful_sentences\n",
    "\n",
    "    # We then create the term-sentence matrix from the cleaned up-text\n",
    "    sentences_df = pd.DataFrame(sentences)\n",
    "    sentences_df['filename'] = this_file\n",
    "    sentences_df['sentence_order'] = sentences_df.index.values\n",
    "    sentences_df.columns = ['sentence','filename','sentence_order']\n",
    "    full_tokenized = []\n",
    "    for i in tokenized:\n",
    "        full_tokenized.extend(i)\n",
    "\n",
    "    ts_matrix = pd.DataFrame(columns = sorted(set(full_tokenized)))\n",
    "    accumulator = []\n",
    "    accumulator.append(ts_matrix)\n",
    "    for i in range(len(tokenized)):\n",
    "        fdist = FreqDist(tokenized[i])\n",
    "        tmp = pd.DataFrame.from_dict(dict(fdist), orient = 'index')\n",
    "        tmp1 = tmp.reset_index()\n",
    "        tmp1.columns = ['word','count']\n",
    "        tmp1 = pd.pivot_table(tmp1, columns = 'word')\n",
    "        tmp1.index = [i]\n",
    "        accumulator.append(tmp1)\n",
    "    ts_matrix = pd.concat(accumulator, ignore_index = True)\n",
    "    # we then remove the stopwords:\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    ts_matrix = ts_matrix[ts_matrix.columns[~ts_matrix.columns.str.lower().isin(stop_words)]]\n",
    "\n",
    "    #adding the file for reference:\n",
    "    ts_matrix['this_file_name'] = this_file\n",
    "    ts_matrix['sentence_order'] = ts_matrix.index.values\n",
    "    \n",
    "\n",
    "    # resulting input matrix: Term-sentence\n",
    "    return(ts_matrix,sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories_list = sorted(glob.glob('/home/joao/Thesis/DUC2002/DUC2002_Summarization_Documents/docs/*'))\n",
    "\n",
    "def generate_ts_matrices_for_all_files(directory):\n",
    "    file_names = sorted(glob.glob(directory + '/*'))\n",
    "    this_directory = directory.split('/')[-1]\n",
    "    ts_matrices = []\n",
    "    sentence_bank = []\n",
    "    for filename in file_names:\n",
    "        with open(filename,'rb') as f:\n",
    "            a = get_ts_matrix(f,filename)\n",
    "            ts_matrices.append(a[0])\n",
    "            sentence_bank.append(a[1])\n",
    "    final_ts_matrix = pd.concat(ts_matrices, ignore_index = True)\n",
    "    final_ts_matrix = final_ts_matrix[list(final_ts_matrix.columns[final_ts_matrix.columns.str.isalpha()])+\n",
    "                                      ['this_file_name','sentence_order']]\n",
    "    sentence_bank = pd.concat(sentence_bank,ignore_index = True)\n",
    "    rows_to_drop = final_ts_matrix.index[final_ts_matrix.drop(columns = ['this_file_name',\n",
    "                                                                         'sentence_order']).fillna(0).sum(axis =1) == 0]\n",
    "    sentence_bank = sentence_bank.drop(index = rows_to_drop).reset_index(drop = True)\n",
    "    final_ts_matrix = final_ts_matrix.drop(index=  rows_to_drop).reset_index(drop = True)\n",
    "    word_count = sentence_bank.sentence.str.split(' ').str.len()\n",
    "    final_ts_matrix['word_count'] = word_count\n",
    "    sentence_bank.to_pickle('/home/joao/Thesis/sentence_bank/'+this_directory+'.p')\n",
    "    final_ts_matrix.to_pickle('/home/joao/Thesis/titles/'+this_directory+'.p')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs = -1, verbose = 11)(delayed(generate_ts_matrices_for_all_files\n",
    "                                                     )(directory)for directory in directories_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Run time: 84 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the abstract ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "directory_lists = sorted(glob.glob('/home/joao/Thesis/DUC2002/extracts_abstracts/d*'))\n",
    "summaries = []\n",
    "files_list = []\n",
    "people_list = []\n",
    "for i in tqdm(directory_lists):\n",
    "    files = glob.glob(i+'/400*')\n",
    "    if(files):\n",
    "        print(i)\n",
    "        with open(files[0],'rb') as f:\n",
    "            a = f.read()\n",
    "\n",
    "        d = re.sub('<[^>]+>', 'AAAAAA', a)\n",
    "        this_summary = i.split('/')[-1][:-1]\n",
    "        this_person = i.split('/')[-1][-1].upper()\n",
    "        this_file = '/home/joao/Thesis/simplified_abstracts/'+this_summary\n",
    "        summaries.append(d)\n",
    "        files_list.append(this_summary)\n",
    "        people_list.append(this_person)\n",
    "#         with open(this_file,'wb') as f:\n",
    "#             f.write(d)\n",
    "# we then save this file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summaries_df = pd.DataFrame({'summaries':summaries,'files':files_list,'person':people_list})\n",
    "\n",
    "summaries_df.files = summaries_df.person + '.' + summaries_df.files.str[1:-1] \n",
    "\n",
    "print(summaries_df.summaries[1])\n",
    "summaries_df.summaries = summaries_df.summaries.str.strip('AAAAAA')\n",
    "\n",
    "summaries_df.summaries = summaries_df.summaries.str.replace('\\n\\n','\\n')\n",
    "summaries_df.summaries = summaries_df.summaries.str.replace('\\r\\n', '')\n",
    "summaries_df.summaries = summaries_df.summaries.str.replace('AAAAAA','\\r\\n')\n",
    "summaries_df.summaries = summaries_df.summaries.str.strip()\n",
    "summaries_df.summaries = summaries_df.summaries.str.replace('\\r\\n\\r\\n','\\r\\n')\n",
    "print('\\r\\n\\r\\n')\n",
    "print(summaries_df.summaries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(summaries_df.summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in summaries_df.index:\n",
    "    this_file = summaries_df.loc[i,:]\n",
    "    this_filename = '/home/joao/Thesis/simplified_abstracts/abstract.' + this_file.files + '.txt'\n",
    "    with open(this_filename,'wb') as f:\n",
    "        f.write(str(this_file.summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminating the starting spaces and comas:\n",
    "a = glob.glob('/home/joao/Thesis/simplified_abstracts/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_file = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(this_file,'rb') as f:\n",
    "    this_abstract = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We must then preprocess the titles as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk.data\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from joblib import Parallel,delayed\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "def get_ts_matrix(f,filename):\n",
    "    \"\"\"This function is responsible for getting the ts matrix \n",
    "        from a given file in the DUC2002 format\n",
    "        inputs: f -> A raw open file in the DUC2002 format\n",
    "        filename --> The path to the file so we can get the name of\n",
    "        the resulting documents outputs: TS-matrix for this file, \n",
    "        removing stopwords and with an added column \n",
    "        containing the filename for use in multiple document summarization\"\"\"\n",
    "    a = BeautifulSoup(f,'html.parser')\n",
    "    print(a.prettify())\n",
    "    tmp = a.find('head')\n",
    "    if(tmp):\n",
    "        text = tmp.text\n",
    "    else:\n",
    "        tmp = a.find('hl')\n",
    "        if(tmp):\n",
    "            text = tmp.text\n",
    "        else:\n",
    "            tmp = a.find('headline')\n",
    "            if(tmp):\n",
    "                text = tmp.text\n",
    "            else:\n",
    "                tmp = a.find('ti')\n",
    "                if(tmp):\n",
    "                    text = tmp.text\n",
    "                else:\n",
    "                    print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "                    print('no title in file')\n",
    "                    print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\\n\\n\\n\\n\\n\\n')\n",
    "                    return pd.DataFrame()\n",
    "    this_file = filename.split(\"/\")[-1]\n",
    "    #we then proceed to preprocess the text:\n",
    "    #first by tokeninzing it\n",
    "    #detecting all the sentences:\n",
    "    sentences = sent_detector.tokenize(text)\n",
    "    #tokeninzing each sentence\n",
    "\n",
    "    tokenized = []\n",
    "    useful_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        if(any(c.isalpha() for c in sentences[i])):\n",
    "            tokenized.append(WordPunctTokenizer().tokenize(sentences[i].strip()))\n",
    "            useful_sentences.append(sentences[i].strip())\n",
    "    sentences = useful_sentences\n",
    "\n",
    "    # We then create the term-sentence matrix from the cleaned up-text\n",
    "    sentences_df = pd.DataFrame(sentences)\n",
    "    sentences_df['filename'] = this_file\n",
    "    sentences_df['sentence_order'] = sentences_df.index.values\n",
    "    sentences_df.columns = ['sentence','filename','sentence_order']\n",
    "    full_tokenized = []\n",
    "    for i in tokenized:\n",
    "        full_tokenized.extend(i)\n",
    "\n",
    "    ts_matrix = pd.DataFrame(columns = sorted(set(full_tokenized)))\n",
    "    accumulator = []\n",
    "    accumulator.append(ts_matrix)\n",
    "    for i in range(len(tokenized)):\n",
    "        fdist = FreqDist(tokenized[i])\n",
    "        tmp = pd.DataFrame.from_dict(dict(fdist), orient = 'index')\n",
    "        tmp1 = tmp.reset_index()\n",
    "        tmp1.columns = ['word','count']\n",
    "        tmp1 = pd.pivot_table(tmp1, columns = 'word')\n",
    "        tmp1.index = [i]\n",
    "        accumulator.append(tmp1)\n",
    "    ts_matrix = pd.concat(accumulator, ignore_index = True\n",
    "    # we then remove the stopwords:\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ts_matrix = ts_matrix[ts_matrix.columns[~ts_matrix.columns.str.lower().isin(stop_words)]]\n",
    "\n",
    "    #adding the file for reference:\n",
    "    ts_matrix['this_file_name'] = this_file\n",
    "    ts_matrix['sentence_order'] = ts_matrix.index.values\n",
    "    \n",
    "\n",
    "    # resulting input matrix: Term-sentence\n",
    "    return(ts_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories_list = sorted(glob.glob('/home/joao/Thesis/DUC2002/DUC2002_Summarization_Documents/docs/*'))\n",
    "def generate_ts_matrices_for_all_files(directory):\n",
    "    file_names = sorted(glob.glob(directory + '/*'))\n",
    "    this_directory = directory.split('/')[-1]\n",
    "    ts_matrices = []\n",
    "    sentence_bank = []\n",
    "    for filename in file_names:\n",
    "        with open(filename,'rb') as f:\n",
    "            a = get_ts_matrix(f,filename)\n",
    "            ts_matrices.append(a)\n",
    "    final_ts_matrix = pd.concat(ts_matrices, ignore_index = True)\n",
    "    final_ts_matrix = final_ts_matrix[list(final_ts_matrix.columns[final_ts_matrix.columns.str.isalpha()])+['this_file_name','sentence_order']]\n",
    "    rows_to_drop = final_ts_matrix.index[final_ts_matrix.drop(columns = ['this_file_name','sentence_order']).fillna(0).sum(axis =1) == 0]\n",
    "    final_ts_matrix = final_ts_matrix.drop(index=  rows_to_drop).reset_index(drop = True)\n",
    "    final_ts_matrix.to_pickle('/home/joao/Thesis/titles/'+this_directory+'.p')\n",
    "    return final_ts_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(directories_list):\n",
    "    generate_ts_matrices_for_all_files(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs = -1, verbose = 11)(delayed(generate_ts_matrices_for_all_files)(directory)for directory in directories_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate = generate_ts_matrices_for_all_files(directories_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(intermediate[1],'rb') as f:\n",
    "    a = BeautifulSoup(f,'html.parser')\n",
    "    text = a.find('head').text.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "filelist = sorted(glob.glob('/home/joao/Thesis/ts_matrices_original/*p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_words = []\n",
    "avg_word_sentence = []\n",
    "avg_docs = []\n",
    "avg_sentences = []\n",
    "avg_word_doc = []\n",
    "for filename in filelist:\n",
    "    tmp = pd.read_pickle(filename)\n",
    "    avg_sentences.append(tmp[['word_count','this_file_name']].groupby('this_file_name').count()['word_count'])\n",
    "    avg_word_sentence.append(tmp.word_count.mean())\n",
    "    avg_docs.append(tmp.this_file_name.unique().shape[0])\n",
    "    avg_word_doc.append(tmp[['word_count','this_file_name']].groupby('this_file_name').sum()['word_count'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(avg_sentences),np.mean(avg_word_sentence),np.mean(avg_docs),np.mean(avg_word_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(30.833333333333332, 19.886486486486486, 6.0, 613.1666666666666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
