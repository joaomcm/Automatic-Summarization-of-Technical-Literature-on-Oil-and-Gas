{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We start by loading an example set of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrix_files = sorted(\n",
    "    glob.glob('/home/joao/Thesis/ts_matrices_original/*.p'))\n",
    "example_file = ts_matrix_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage(example_bin, word, word_set):\n",
    "    if (not word_set):\n",
    "        return 0\n",
    "    else:\n",
    "        # we take the subset of phrases containing the word set\n",
    "        # and the word\n",
    "        if (len(word_set) > 1):\n",
    "            sentence_subset = example_bin.loc[:, word_set]\n",
    "            word_vec = example_bin.loc[:, word]\n",
    "            repeated_vector = np.tile(\n",
    "                word_vec, (sentence_subset.shape[1], 1)).transpose()\n",
    "            coverage = np.logical_and(sentence_subset, repeated_vector)\n",
    "            coverage = (coverage.sum(axis=1) > 0).sum() / word_vec.sum()\n",
    "        else:\n",
    "            sentence_subset = example_bin.loc[:, word_set[0]]\n",
    "            word_vec = example_bin.loc[:, word]\n",
    "            coverage = np.logical_and(sentence_subset,\n",
    "                                      word_vec).sum() / word_vec.sum()\n",
    "\n",
    "        return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsuming_graphs(example_file):\n",
    "    example = pd.read_pickle(example_file)\n",
    "    keys = example[['this_file_name', 'sentence_order', 'word_count']]\n",
    "    example.drop(\n",
    "        columns=['this_file_name', 'sentence_order', 'word_count'],\n",
    "        inplace=True)\n",
    "\n",
    "    example_bin = example > 0\n",
    "\n",
    "    # we then define the SPAN as being the number of\n",
    "    # sentences containing a given word\n",
    "    SPAN = example_bin.sum(axis=0)\n",
    "\n",
    "    #we then define a function that gets the coverage\n",
    "    #of a word w given a word set word_set:\n",
    "\n",
    "    # we must now define the subsuming relationships\n",
    "    #between the variables:\n",
    "    # We then sort the words by SPAN\n",
    "    sorted_span = SPAN.sort_values(ascending=False)\n",
    "    subsuming_candidates = sorted_span[sorted_span > 1].index.values\n",
    "    subsuming_candidates_no = subsuming_candidates.shape[0]\n",
    "    lambda_2 = 0.75\n",
    "    lambda_1_bar = 0.55\n",
    "\n",
    "    # we now start defining subsuming relationships for all the\n",
    "    # words in the file\n",
    "    #initially hecka fucking slow - so let's do some heuristics\n",
    "    \"\"\"Corolary: No word with SPAN <= 1 can possibly subsume another.\n",
    "    \"\"\"\n",
    "    subsuming_dict = {sorted_span.index[0]: []}\n",
    "    for i in tqdm(range(1, sorted_span.shape[0])):\n",
    "        current_word = sorted_span.index[i]\n",
    "        max_cov = []\n",
    "        max_cov = Parallel(n_jobs=8)(\n",
    "            delayed(get_coverage)(example_bin, current_word, [candidate_word])\n",
    "            for candidate_word in\n",
    "            subsuming_candidates[0:min(i, subsuming_candidates_no)])\n",
    "        max_cov = max(max_cov)\n",
    "        lambda_1 = lambda_1_bar * max_cov\n",
    "        for j in range(0, min(i, subsuming_candidates_no)):\n",
    "            candidate_word = sorted_span.index[j]\n",
    "            subsuming_j = subsuming_dict[candidate_word]\n",
    "            condition_1 = get_coverage(example_bin, current_word,\n",
    "                                       [candidate_word]) >= lambda_1\n",
    "            condition_2 = get_coverage(example_bin, current_word,\n",
    "                                       subsuming_j) < lambda_2\n",
    "            if (condition_1 & condition_2):\n",
    "                subsuming_j.append(current_word)\n",
    "                subsuming_dict[candidate_word] = subsuming_j\n",
    "        subsuming_dict.update({current_word: []})\n",
    "    filename = ('/home/joao/Thesis/progressive/subsuming_graphs/' +\n",
    "                example_file.split('/')[-1])\n",
    "    pickle.dump(subsuming_dict, open(filename, 'wb'))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=8)(delayed(get_subsuming_graphs)(example_file)\n",
    "                   for example_file in ts_matrix_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time needed for execution\n",
    "CPU times: user 698 ms, sys: 536 ms, total: 1.23 s\n",
    "Wall time: 9h 19min 25s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part: Building the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading an example graph:\n",
    "%load_ext line_profiler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "import pickle\n",
    "a = glob.glob('/home/joao/Thesis/progressive/subsuming_graphs/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_graph = pickle.load(open(a[0], 'rb'))\n",
    "\n",
    "# we then create the subsuming graph\n",
    "DG = nx.DiGraph()\n",
    "\n",
    "DG = nx.DiGraph(example_graph)\n",
    "# we must also load the ts matrix:\n",
    "ts_matrix_file = (\n",
    "    '/home/joao/Thesis/ts_matrices_original/' + a[0].split('/')[-1])\n",
    "example = pd.read_pickle(ts_matrix_file)\n",
    "keys = example[['this_file_name', 'sentence_order']]\n",
    "example.drop(columns=['this_file_name', 'sentence_order'], inplace=True)\n",
    "\n",
    "example_bin = example > 0\n",
    "\n",
    "# we then define the SPAN as being the number of sentences\n",
    "# containing a given word\n",
    "SPAN = example_bin.sum(axis=0)\n",
    "# we also load the full sentence bank:\n",
    "sentence_bank_file = ('/home/joao/Thesis/sentence_bank/' + a[0].split('/')[-1])\n",
    "sentence_bank = pd.read_pickle(sentence_bank_file)\n",
    "\n",
    "# we then define the damping factor, alfa\n",
    "alfa = 0.5\n",
    "\n",
    "# we now start defining the sentence selection algorithm\n",
    "#first, we define the basic words - nodes with indegree = 0\n",
    "full_word_list = example_bin.columns.values\n",
    "degree_dict = dict(DG.in_degree(full_word_list))\n",
    "degree_df = pd.DataFrame({\n",
    "    'word': degree_dict.keys(),\n",
    "    'in_degree': degree_dict.values()\n",
    "})\n",
    "general_words = degree_df.loc[degree_df.in_degree == 0, 'word']\n",
    "# we then define the sentence_sets:\n",
    "sentence_sets = []\n",
    "for i in example_bin.index:\n",
    "    tmp = example_bin.loc[i, :]\n",
    "    sentence_sets.append(tmp[tmp > 0].index.values)\n",
    "\n",
    "\n",
    "# we then define the function that gets the conditional\n",
    "# saliency for a sentence given another:\n",
    "def get_conditional_saliency(word_set_1, word_set_2, SPAN, DG):\n",
    "    cs = 0\n",
    "    sub_df = list(word_set_1) + list(word_set_2)\n",
    "    subset_dg = DG.subgraph(sub_df)\n",
    "    for i in word_set_1:\n",
    "        for j in word_set_2:\n",
    "            if (nx.has_path(subset_dg, j, i)):\n",
    "                cs += np.log(SPAN[i])\n",
    "                break\n",
    "    return cs\n",
    "\n",
    "\n",
    "total_cs = []\n",
    "sentence_nums = []\n",
    "for sentence_num in tqdm(range(len(sentence_sets))):\n",
    "    this_sentence = sentence_sets[sentence_num]\n",
    "    total_cs.append(\n",
    "        get_conditional_saliency(this_sentence, general_words, SPAN, DG))\n",
    "    sentence_nums.append(sentence_num)\n",
    "final_cs = pd.DataFrame({'sentence_num': sentence_nums, 'cs': total_cs})\n",
    "\n",
    "first_sentence = final_cs[final_cs.cs == final_cs.cs.max()].sentence_num\n",
    "\n",
    "selected_sentence_keys = keys.iloc[first_sentence]\n",
    "\n",
    "selected_sentence_keys\n",
    "this_text = sentence_bank[sentence_bank.filename == selected_sentence_keys.\n",
    "                          this_file_name.values[0]]\n",
    "\n",
    "this_sentence = this_text[this_text.sentence_order == selected_sentence_keys.\n",
    "                          sentence_order.values[0]].sentence.values\n",
    "\n",
    "abstract = list(this_sentence)\n",
    "selected_sentence_nums = [first_sentence.values[0]]\n",
    "selected_sentences = [sentence_sets[first_sentence.values[0]]]\n",
    "\n",
    "total_abstract_length = len(abstract[-1][0].split(' '))\n",
    "# we then update SPAN\n",
    "SPAN[selected_sentences[-1]] = SPAN[selected_sentences[-1]] * alfa\n",
    "\n",
    "# we may then start the progressive summarization procedure -\n",
    "# using only 400 words\n",
    "while (total_abstract_length < 400):\n",
    "    cs = []\n",
    "    sentence_nums = []\n",
    "    for sentence_num in range(len(sentence_sets)):\n",
    "        this_sentence = sentence_sets[sentence_num]\n",
    "        potential_cs = []\n",
    "        for present_sentence in selected_sentences:\n",
    "            potential_cs.append(\n",
    "                get_conditional_saliency(this_sentence, present_sentence, SPAN,\n",
    "                                         DG))\n",
    "        cs.append(max(potential_cs))\n",
    "        sentence_nums.append(sentence_num)\n",
    "    final_cs = pd.DataFrame({'sentence_num': sentence_nums, 'cs': total_cs})\n",
    "    #we ignore already selected sentences\n",
    "    final_cs = final_cs[~final_cs.sentence_num.isin(selected_sentence_nums)]\n",
    "    next_sentence = final_cs[final_cs.cs == final_cs.cs.max()].sentence_num\n",
    "    if (next_sentence.shape[0] != 1):\n",
    "        selected_sentence_keys = keys.iloc[next_sentence.values]\n",
    "        # we select the sentence that came first\n",
    "        selected_sentence_keys = selected_sentence_keys[\n",
    "            selected_sentence_keys.sentence_order == selected_sentence_keys.\n",
    "            sentence_order.min()]\n",
    "        next_sentence = selected_sentence_keys.index.values[0]\n",
    "    else:\n",
    "        next_sentence = next_sentence.values[0]\n",
    "        selected_sentence_keys = keys.iloc[next_sentence]\n",
    "        selected_sentence_keys = keys.iloc[next_sentence]\n",
    "    print(next_sentence)\n",
    "    this_text = sentence_bank[sentence_bank.filename == selected_sentence_keys.\n",
    "                              this_file_name]\n",
    "    this_sentence = this_text[this_text.sentence_order ==\n",
    "                              selected_sentence_keys.\n",
    "                              sentence_order].sentence.values[0]\n",
    "    remaining_words = 400 - total_abstract_length\n",
    "    if (len(this_sentence.split(' ')) > remaining_words):\n",
    "        break\n",
    "    else:\n",
    "        abstract.append(this_sentence)\n",
    "        selected_sentence_nums.append(next_sentence)\n",
    "        selected_sentences.append(sentence_sets[next_sentence])\n",
    "        total_abstract_length += len(abstract[-1].split(' '))\n",
    "        SPAN[selected_sentences[-1]] = SPAN[selected_sentences[-1]] * alfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the abstract:\n",
    "final_filename = ('/home/joao/Thesis/progressive/abstracts/' +\n",
    "                  a[0].split('/')[-1][:-2] + '.txt')\n",
    "with open(final_filename, 'wb') as f:\n",
    "    for i in abstract:\n",
    "        f.write(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now turn it into a function and paralellize its execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading an example graph:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "import pickle\n",
    "graph_files = pd.Series(\n",
    "    sorted(glob.glob('/home/joao/Thesis/progressive/subsuming_graphs/*')))\n",
    "done_files = pd.Series(\n",
    "    sorted(glob.glob('/home/joao/Thesis/progressive/abstracts/*')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_abstract(graph_file):\n",
    "    example_graph = pickle.load(open(graph_file, 'rb'))\n",
    "\n",
    "    # we then create the subsuming graph\n",
    "    DG = nx.DiGraph()\n",
    "\n",
    "    DG = nx.DiGraph(example_graph)\n",
    "    # we must also load the ts matrix:\n",
    "    ts_matrix_file = '/home/joao/Thesis/ts_matrices_original/' + graph_file.split(\n",
    "        '/')[-1]\n",
    "    example = pd.read_pickle(ts_matrix_file)\n",
    "    keys = example[['this_file_name', 'sentence_order', 'word_count']]\n",
    "    example.drop(\n",
    "        columns=['this_file_name', 'sentence_order', 'word_count'],\n",
    "        inplace=True)\n",
    "\n",
    "    example_bin = example > 0\n",
    "\n",
    "    # we then define the SPAN as being the number of sentences containing a given word\n",
    "    SPAN = example_bin.sum(axis=0)\n",
    "    # we also load the full sentence bank:\n",
    "    sentence_bank_file = '/home/joao/Thesis/sentence_bank/' + graph_file.split(\n",
    "        '/')[-1]\n",
    "    sentence_bank = pd.read_pickle(sentence_bank_file)\n",
    "\n",
    "    # we then define the damping factor, alfa\n",
    "    alfa = 0.5\n",
    "\n",
    "    # we now start defining the sentence selection algorithm\n",
    "    #first, we define the basic words - nodes with indegree = 0\n",
    "    full_word_list = example_bin.columns.values\n",
    "    degree_dict = dict(DG.in_degree(full_word_list))\n",
    "    degree_df = pd.DataFrame({\n",
    "        'word': degree_dict.keys(),\n",
    "        'in_degree': degree_dict.values()\n",
    "    })\n",
    "    general_words = degree_df.loc[degree_df.in_degree == 0, 'word']\n",
    "    # we then define the sentence_sets:\n",
    "    sentence_sets = []\n",
    "    for i in example_bin.index:\n",
    "        tmp = example_bin.loc[i, :]\n",
    "        sentence_sets.append(tmp[tmp > 0].index.values)\n",
    "\n",
    "    # we then define the function that gets the conditional saliency\n",
    "    #for a sentence given another:\n",
    "    def get_conditional_saliency(word_set_1, word_set_2, SPAN, DG):\n",
    "        cs = 0\n",
    "        sub_df = list(word_set_1) + list(word_set_2)\n",
    "        subset_dg = DG.subgraph(sub_df)\n",
    "        for i in word_set_1:\n",
    "            for j in word_set_2:\n",
    "                if (nx.has_path(subset_dg, j, i)):\n",
    "                    cs += np.log(SPAN[i])\n",
    "                    break\n",
    "        return cs\n",
    "\n",
    "    total_cs = []\n",
    "    sentence_nums = []\n",
    "    for sentence_num in tqdm(range(len(sentence_sets))):\n",
    "        this_sentence = sentence_sets[sentence_num]\n",
    "        total_cs.append(\n",
    "            get_conditional_saliency(this_sentence, general_words, SPAN, DG))\n",
    "        sentence_nums.append(sentence_num)\n",
    "    final_cs = pd.DataFrame({'sentence_num': sentence_nums, 'cs': total_cs})\n",
    "\n",
    "    first_sentence = final_cs[final_cs.cs == final_cs.cs.max()].sentence_num\n",
    "\n",
    "    selected_sentence_keys = keys.iloc[first_sentence]\n",
    "\n",
    "    selected_sentence_keys\n",
    "    this_text = sentence_bank[sentence_bank.filename == selected_sentence_keys.\n",
    "                              this_file_name.values[0]]\n",
    "\n",
    "    this_sentence = this_text[this_text.sentence_order ==\n",
    "                              selected_sentence_keys.sentence_order.\n",
    "                              values[0]].sentence.values\n",
    "\n",
    "    abstract = list(this_sentence)\n",
    "    selected_sentence_nums = [first_sentence.values[0]]\n",
    "    selected_sentences = [sentence_sets[first_sentence.values[0]]]\n",
    "\n",
    "    total_abstract_length = len(abstract[-1][0].split(' '))\n",
    "    # we then update SPAN\n",
    "    SPAN[selected_sentences[-1]] = SPAN[selected_sentences[-1]] * alfa\n",
    "\n",
    "    # we may then start the progressive summarization procedure -\n",
    "    # using only 400 words\n",
    "    while (total_abstract_length < 400):\n",
    "        cs = []\n",
    "        sentence_nums = []\n",
    "        for sentence_num in range(len(sentence_sets)):\n",
    "            this_sentence = sentence_sets[sentence_num]\n",
    "            potential_cs = []\n",
    "            for present_sentence in selected_sentences:\n",
    "                potential_cs.append(\n",
    "                    get_conditional_saliency(this_sentence, present_sentence,\n",
    "                                             SPAN, DG))\n",
    "            cs.append(max(potential_cs))\n",
    "            sentence_nums.append(sentence_num)\n",
    "        final_cs = pd.DataFrame({\n",
    "            'sentence_num': sentence_nums,\n",
    "            'cs': total_cs\n",
    "        })\n",
    "        #we ignore already selected sentences\n",
    "        final_cs = final_cs[~final_cs.sentence_num.\n",
    "                            isin(selected_sentence_nums)]\n",
    "        next_sentence = final_cs[final_cs.cs == final_cs.cs.max()].sentence_num\n",
    "        if (next_sentence.shape[0] != 1):\n",
    "            selected_sentence_keys = keys.iloc[next_sentence.values]\n",
    "            # we select the sentence that came first\n",
    "            selected_sentence_keys = selected_sentence_keys[\n",
    "                selected_sentence_keys.sentence_order ==\n",
    "                selected_sentence_keys.sentence_order.min()]\n",
    "            next_sentence = selected_sentence_keys.index.values[0]\n",
    "            selected_sentence_keys = keys.iloc[next_sentence]\n",
    "\n",
    "        else:\n",
    "            next_sentence = next_sentence.values[0]\n",
    "            selected_sentence_keys = keys.iloc[next_sentence]\n",
    "\n",
    "\n",
    "        this_text = sentence_bank[sentence_bank.filename ==\n",
    "                                  selected_sentence_keys.this_file_name]\n",
    "        this_sentence = this_text[this_text.sentence_order ==\n",
    "                                  selected_sentence_keys.\n",
    "                                  sentence_order].sentence.values[0]\n",
    "        remaining_words = 400 - total_abstract_length\n",
    "        if (len(this_sentence.split(' ')) > remaining_words):\n",
    "            break\n",
    "        else:\n",
    "            abstract.append(this_sentence)\n",
    "            selected_sentence_nums.append(next_sentence)\n",
    "            selected_sentences.append(sentence_sets[next_sentence])\n",
    "            total_abstract_length += len(abstract[-1].split(' '))\n",
    "            SPAN[selected_sentences[-1]] = SPAN[selected_sentences[-1]] * alfa\n",
    "    #saving the abstract:\n",
    "    final_filename = '/home/joao/Thesis/progressive/abstracts/' + graph_file.split(\n",
    "        '/')[-1][:-2] + '.txt'\n",
    "    with open(final_filename, 'wb') as f:\n",
    "        for i in abstract:\n",
    "            f.write(str(i).strip())\n",
    "            if (i != abstract[-1]):\n",
    "                f.write('\\r\\n')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(\n",
    "    n_jobs=8, verbose=11)(\n",
    "        delayed(create_abstract)(graph_file) for graph_file in graph_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract Generation time : 45.1 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting the abstracts in the right name format:\n",
    "a = glob.glob('/home/joao/Thesis/progressive/abstracts/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a:\n",
    "    this_file = '/home/joao/Thesis/progressive/right_name/abstract.' + i.split(\n",
    "        '/')[-1][1:4] + '.txt'\n",
    "    with open(i, 'rb') as f:\n",
    "        abstract = f.read()\n",
    "    with open(this_file, 'wb') as f:\n",
    "        f.write(abstract.stri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the trailing \\r\\n\n",
    "b = glob.glob('/home/joao/Thesis/progressive/right_name/*.txt')\n",
    "for i in b:\n",
    "    with open(i, 'rb') as f:\n",
    "        abstract = f.read()\n",
    "    with open(i, 'wb') as f:\n",
    "        f.write(abstract[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge, FilesRouge\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then load all summaries and candidate summaries:\n",
    "auto_summaries = glob.glob('/home/joao/Thesis/progressive/right_name/*')\n",
    "true_summaries = pd.Series(\n",
    "    sorted(glob.glob('/home/joao/Thesis/simplified_abstracts/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = []\n",
    "for i in tqdm(auto_summaries):\n",
    "    this_file = i.split('/')[-1]\n",
    "    this_file_num = this_file.split('.')[-2]\n",
    "    ground_truths = true_summaries[true_summaries.str[-7:-4] == this_file_num]\n",
    "    scores = []\n",
    "    rouge = Rouge()\n",
    "    with open(i, 'rb') as f:\n",
    "        auto_summary = f.read()\n",
    "    for j in ground_truths:\n",
    "        with open(j, 'rb') as f:\n",
    "            ground_truth = f.read()\n",
    "            tmp_scores = rouge.get_scores(auto_summary, ground_truth, avg=True)\n",
    "        scores.append(tmp_scores['rouge-2']['p'])\n",
    "    total_scores.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we now transform the function to allow it to be implemented for the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "ts_matrix_files = sorted(\n",
    "    glob.glob('/home/joao/Thesis/test_set/ts_matrices/*.p'))\n",
    "example_file = ts_matrix_files[0]\n",
    "\n",
    "\n",
    "def get_coverage(example_bin, word, word_set):\n",
    "    if (not word_set):\n",
    "        return 0\n",
    "    else:\n",
    "        # we take the subset of phrases containing the word\n",
    "        # set and the word\n",
    "        if (len(word_set) > 1):\n",
    "            sentence_subset = example_bin.loc[:, word_set]\n",
    "            word_vec = example_bin.loc[:, word]\n",
    "            repeated_vector = np.tile(\n",
    "                word_vec, (sentence_subset.shape[1], 1)).transpose()\n",
    "            coverage = np.logical_and(sentence_subset, repeated_vector)\n",
    "            coverage = (coverage.sum(axis=1) > 0).sum() / word_vec.sum()\n",
    "        else:\n",
    "            sentence_subset = example_bin.loc[:, word_set[0]]\n",
    "            word_vec = example_bin.loc[:, word]\n",
    "            coverage = np.logical_and(sentence_subset,\n",
    "                                      word_vec).sum() / word_vec.sum()\n",
    "\n",
    "        return coverage\n",
    "\n",
    "\n",
    "def get_subsuming_graphs(example_file):\n",
    "    example = pd.read_pickle(example_file)\n",
    "    keys = example[['this_file_name', 'sentence_order', 'word_count']]\n",
    "    example.drop(\n",
    "        columns=['this_file_name', 'sentence_order', 'word_count'],\n",
    "        inplace=True)\n",
    "\n",
    "    example_bin = example > 0\n",
    "\n",
    "    # we then define the SPAN as being the number of sentences containing a given word\n",
    "    SPAN = example_bin.sum(axis=0)\n",
    "\n",
    "    #we then define a function that gets the coverage of a word w given a word set word_set:\n",
    "\n",
    "    # we must now define the subsuming relationships between the variables:\n",
    "    # We then sort the words by SPAN\n",
    "    sorted_span = SPAN.sort_values(ascending=False)\n",
    "    subsuming_candidates = sorted_span[sorted_span > 1].index.values\n",
    "    subsuming_candidates_no = subsuming_candidates.shape[0]\n",
    "    lambda_2 = 0.75\n",
    "    lambda_1_bar = 0.55\n",
    "\n",
    "    # we now start defining subsuming relationships for all the words in the file\n",
    "    #initially hecka fucking slow - so let's do some heuristics\n",
    "    \"\"\"Corolary: No word with SPAN <= 1 can possibly subsume another.\n",
    "    \"\"\"\n",
    "    subsuming_dict = {sorted_span.index[0]: []}\n",
    "    for i in tqdm(range(1, sorted_span.shape[0])):\n",
    "        current_word = sorted_span.index[i]\n",
    "        max_cov = []\n",
    "        max_cov = Parallel(n_jobs=8)(\n",
    "            delayed(get_coverage)(example_bin, current_word, [candidate_word])\n",
    "            for candidate_word in\n",
    "            subsuming_candidates[0:min(i, subsuming_candidates_no)])\n",
    "        max_cov = max(max_cov)\n",
    "        lambda_1 = lambda_1_bar * max_cov\n",
    "        for j in range(0, min(i, subsuming_candidates_no)):\n",
    "            candidate_word = sorted_span.index[j]\n",
    "            subsuming_j = subsuming_dict[candidate_word]\n",
    "            condition_1 = get_coverage(example_bin, current_word,\n",
    "                                       [candidate_word]) >= lambda_1\n",
    "            condition_2 = get_coverage(example_bin, current_word,\n",
    "                                       subsuming_j) < lambda_2\n",
    "            if (condition_1 & condition_2):\n",
    "                subsuming_j.append(current_word)\n",
    "                subsuming_dict[candidate_word] = subsuming_j\n",
    "        subsuming_dict.update({current_word: []})\n",
    "    filename = '/home/joao/Thesis/progressive/test_subsuming_graphs/' + example_file.split(\n",
    "        '/')[-1]\n",
    "    pickle.dump(subsuming_dict, open(filename, 'wb'))\n",
    "    return 0\n",
    "\n",
    "\n",
    "Parallel(\n",
    "    n_jobs=8, verbose=11)(delayed(get_subsuming_graphs)(example_file)\n",
    "                          for example_file in ts_matrix_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXECUTION TIME: 1h 19min 34s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now creating the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading an example graph:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "import pickle\n",
    "graph_files = pd.Series(\n",
    "    sorted(glob.glob('/home/joao/Thesis/progressive/test_subsuming_graphs/*')))\n",
    "done_files = pd.Series(\n",
    "    sorted(glob.glob('/home/joao/Thesis/progressive/test_abstracts/*')))\n",
    "\n",
    "\n",
    "\n",
    "def create_abstract(graph_file):\n",
    "    example_graph = pickle.load(open(graph_file, 'rb'))\n",
    "\n",
    "    # we then create the subsuming graph\n",
    "    DG = nx.DiGraph()\n",
    "\n",
    "    DG = nx.DiGraph(example_graph)\n",
    "    # we must also load the ts matrix:\n",
    "    ts_matrix_file = '/home/joao/Thesis/test_set/ts_matrices/' + graph_file.split(\n",
    "        '/')[-1]\n",
    "    example = pd.read_pickle(ts_matrix_file)\n",
    "    keys = example[['this_file_name', 'sentence_order', 'word_count']]\n",
    "    example.drop(\n",
    "        columns=['this_file_name', 'sentence_order', 'word_count'],\n",
    "        inplace=True)\n",
    "\n",
    "    example_bin = example > 0\n",
    "\n",
    "    # we then define the SPAN as being the number of sentences containing a given word\n",
    "    SPAN = example_bin.sum(axis=0)\n",
    "    # we also load the full sentence bank:\n",
    "    sentence_bank_file = '/home/joao/Thesis/test_set/sentence_banks/' + graph_file.split(\n",
    "        '/')[-1]\n",
    "    sentence_bank = pd.read_pickle(sentence_bank_file)\n",
    "\n",
    "    # we then define the damping factor, alfa\n",
    "    alfa = 0.5\n",
    "\n",
    "    # we now start defining the sentence selection algorithm\n",
    "    #first, we define the basic words - nodes with indegree = 0\n",
    "    full_word_list = example_bin.columns.values\n",
    "    degree_dict = dict(DG.in_degree(full_word_list))\n",
    "    degree_df = pd.DataFrame({\n",
    "        'word': degree_dict.keys(),\n",
    "        'in_degree': degree_dict.values()\n",
    "    })\n",
    "    general_words = degree_df.loc[degree_df.in_degree == 0, 'word']\n",
    "    # we then define the sentence_sets:\n",
    "    sentence_sets = []\n",
    "    for i in example_bin.index:\n",
    "        tmp = example_bin.loc[i, :]\n",
    "        sentence_sets.append(tmp[tmp > 0].index.values)\n",
    "\n",
    "    # we then define the function that gets the conditional\n",
    "    # saliency for a sentence given another:\n",
    "    def get_conditional_saliency(word_set_1, word_set_2, SPAN, DG):\n",
    "        cs = 0\n",
    "        sub_df = list(word_set_1) + list(word_set_2)\n",
    "        subset_dg = DG.subgraph(sub_df)\n",
    "        for i in word_set_1:\n",
    "            for j in word_set_2:\n",
    "                if (nx.has_path(subset_dg, j, i)):\n",
    "                    cs += np.log(SPAN[i])\n",
    "                    break\n",
    "        return cs\n",
    "\n",
    "    total_cs = []\n",
    "    sentence_nums = []\n",
    "    for sentence_num in tqdm(range(len(sentence_sets))):\n",
    "        this_sentence = sentence_sets[sentence_num]\n",
    "        total_cs.append(\n",
    "            get_conditional_saliency(this_sentence, general_words, SPAN, DG))\n",
    "        sentence_nums.append(sentence_num)\n",
    "    final_cs = pd.DataFrame({'sentence_num': sentence_nums, 'cs': total_cs})\n",
    "\n",
    "    first_sentence = final_cs[final_cs.cs == final_cs.cs.max()].sentence_num\n",
    "\n",
    "    selected_sentence_keys = keys.iloc[first_sentence]\n",
    "\n",
    "    selected_sentence_keys\n",
    "    this_text = sentence_bank[sentence_bank.filename == selected_sentence_keys.\n",
    "                              this_file_name.values[0]]\n",
    "\n",
    "    this_sentence = this_text[this_text.sentence_order ==\n",
    "                              selected_sentence_keys.sentence_order.\n",
    "                              values[0]].sentence.values\n",
    "\n",
    "    abstract = list(this_sentence)\n",
    "    selected_sentence_nums = [first_sentence.values[0]]\n",
    "    selected_sentences = [sentence_sets[first_sentence.values[0]]]\n",
    "\n",
    "    total_abstract_length = len(abstract[-1][0].split(' '))\n",
    "    # we then update SPAN\n",
    "    SPAN[selected_sentences[-1]] = SPAN[selected_sentences[-1]] * alfa\n",
    "\n",
    "    # we may then start the progressive summarization procedure - using only 400 words\n",
    "    while (total_abstract_length < 250):\n",
    "        cs = []\n",
    "        sentence_nums = []\n",
    "        for sentence_num in range(len(sentence_sets)):\n",
    "            this_sentence = sentence_sets[sentence_num]\n",
    "            potential_cs = []\n",
    "            for present_sentence in selected_sentences:\n",
    "                potential_cs.append(\n",
    "                    get_conditional_saliency(this_sentence, present_sentence,\n",
    "                                             SPAN, DG))\n",
    "            cs.append(max(potential_cs))\n",
    "            sentence_nums.append(sentence_num)\n",
    "        final_cs = pd.DataFrame({\n",
    "            'sentence_num': sentence_nums,\n",
    "            'cs': total_cs\n",
    "        })\n",
    "        #we ignore already selected sentences\n",
    "        final_cs = final_cs[~final_cs.sentence_num.\n",
    "                            isin(selected_sentence_nums)]\n",
    "        next_sentence = final_cs[final_cs.cs == final_cs.cs.max()].sentence_num\n",
    "        if (next_sentence.shape[0] != 1):\n",
    "            selected_sentence_keys = keys.iloc[next_sentence.values]\n",
    "            # we select the sentence that came first\n",
    "            selected_sentence_keys = selected_sentence_keys[\n",
    "                selected_sentence_keys.sentence_order ==\n",
    "                selected_sentence_keys.sentence_order.min()]\n",
    "            next_sentence = selected_sentence_keys.index.values[0]\n",
    "            selected_sentence_keys = keys.iloc[next_sentence]\n",
    "\n",
    "        else:\n",
    "            next_sentence = next_sentence.values[0]\n",
    "            selected_sentence_keys = keys.iloc[next_sentence]\n",
    "\n",
    "\n",
    "        this_text = sentence_bank[sentence_bank.filename ==\n",
    "                                  selected_sentence_keys.this_file_name]\n",
    "        this_sentence = this_text[this_text.sentence_order ==\n",
    "                                  selected_sentence_keys.\n",
    "                                  sentence_order].sentence.values[0]\n",
    "        remaining_words = 250 - total_abstract_length\n",
    "        if (len(this_sentence.split(' ')) > remaining_words):\n",
    "            break\n",
    "        else:\n",
    "            abstract.append(this_sentence)\n",
    "            selected_sentence_nums.append(next_sentence)\n",
    "            selected_sentences.append(sentence_sets[next_sentence])\n",
    "            total_abstract_length += len(abstract[-1].split(' '))\n",
    "            SPAN[selected_sentences[-1]] = SPAN[selected_sentences[-1]] * alfa\n",
    "    #saving the abstract:\n",
    "    final_filename = '/home/joao/Thesis/progressive/abstracts/' + graph_file.split(\n",
    "        '/')[-1][:-2] + '.txt'\n",
    "    with open(final_filename, 'wb') as f:\n",
    "        for i in abstract:\n",
    "            f.write(str(i).strip())\n",
    "            if (i != abstract[-1]):\n",
    "                f.write('\\r\\n')\n",
    "    final_abstract = ''\n",
    "    for i in abstract:\n",
    "        final_abstract += i\n",
    "    return (ts_matrix_file, final_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = Parallel(\n",
    "    n_jobs=8, verbose=11)(\n",
    "        delayed(create_abstract)(graph_file) for graph_file in graph_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution time:  5min 59s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "abstracts = []\n",
    "for i in final_results:\n",
    "    filenames.append(i[0])\n",
    "    abstracts.append(i[1])\n",
    "final_df = pd.DataFrame({'filename': filenames, 'abstracts': abstracts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle('/home/joao/Thesis/progressive/final_test_results.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:03<00:00, 21.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge, FilesRouge\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "results_df = pd.read_pickle(\n",
    "    '/home/joao/Thesis/progressive/final_test_results.p')\n",
    "\n",
    "results_df.filename = results_df.filename.str.split('/', expand=True)[6]\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"This function removes all non-ascii characters from text and replaces \n",
    "    them with their closest ascii representation\"\"\"\n",
    "    return unidecode(unicode(text, encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "# we then load all summaries and candidate summaries:\n",
    "\n",
    "total_scores = []\n",
    "scores = []\n",
    "r1 = []\n",
    "r2 = []\n",
    "rl = []\n",
    "for i in tqdm(results_df.index):\n",
    "    ground_truth = '/home/joao/Thesis/test_set/abstracts/ground_truths/' + results_df.loc[\n",
    "        i, 'filename'][:-2] + '.txt'\n",
    "    rouge = Rouge()\n",
    "    with open(ground_truth, 'rb') as f:\n",
    "        ground_truth = f.read()\n",
    "    ground_truth = remove_non_ascii(ground_truth)\n",
    "    tmp_scores = rouge.get_scores(\n",
    "        results_df.loc[i, 'abstracts'], ground_truth, avg=True)\n",
    "    r2.append(tmp_scores['rouge-2']['f'])\n",
    "    r1.append(tmp_scores['rouge-1']['f'])\n",
    "    rl.append(tmp_scores['rouge-l']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('r1', np.mean(r1), np.std(r1, ddof=1))\n",
    "print('r2', np.mean(r2), np.std(r2, ddof=1))\n",
    "print('rl', np.mean(rl), np.std(rl, ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge, FilesRouge\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "results_df = pd.read_pickle(\n",
    "    '/home/joao/Thesis/progressive/final_test_results.p')\n",
    "results_df['r1'] = r1\n",
    "results_df['r2'] = r2\n",
    "results_df['rl'] = rl\n",
    "results_df.abstracts = results_df.abstracts.str.replace('\\n',' ').str.replace('\\r',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('/home/joao/Thesis/progressive/final_test_results.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
