{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from scipy.special import digamma, gamma\n",
    "from scipy.stats import dirichlet\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training set Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then begin the iterative procedure:\n",
    "def eq_3(U,B,V,Y):\n",
    "    denominator = np.matmul(np.matmul(B,U),V.transpose())\n",
    "    mid_term = Y/denominator\n",
    "    tmp  = np.dot(np.dot(B.transpose(),mid_term),V)*U\n",
    "    new_U = tmp/tmp.sum(axis = 0)\n",
    "    return new_U\n",
    "def eq_4(U,B,V,Y,alfa):\n",
    "    denominator = np.matmul(np.matmul(B,U),V.transpose())\n",
    "    mid_term  = (Y/denominator).transpose()\n",
    "    X = np.dot(mid_term,np.matmul(B,U))*V\n",
    "    new_V  = dirichlet_adjustment(X,alfa)\n",
    "    return new_V\n",
    "def dirichlet_adjustment(X,alfa):\n",
    "    #pre_creating_the_result_matrix:\n",
    "    # creating the auxiliary list:\n",
    "    X_alfa = X + np.repeat(np.reshape(alfa,(1,alfa.shape[0])), X.shape[0],axis = 0)\n",
    "    sums = np.repeat(np.reshape(X_alfa.sum(axis = 1),(X_alfa.shape[0],1)),X_alfa.shape[1],axis = 1)\n",
    "    new_V = np.exp(digamma(X_alfa) - digamma(sums))\n",
    "    new_V = new_V/new_V.sum(axis = 1)[:,np.newaxis]\n",
    "    return new_V\n",
    "\n",
    "def get_B(alfa):\n",
    "    return(np.prod(gamma(alfa))/gamma(np.sum(alfa)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrix_list = sorted(glob.glob('/home/joao/Thesis/ts_matrices_original/*.p'))\n",
    "\n",
    "def get_u_v_matrices(this_file):\n",
    "# we start by loading an example term_sentence matrix\n",
    "    term_sentence_document_matrix = pd.read_pickle(this_file)\n",
    "    #from it we create the term_document matrix and the term-sentence matrix\n",
    "    B = term_sentence_document_matrix.copy()\n",
    "    B_keys = B[['this_file_name','sentence_order','word_count']]\n",
    "    B.drop(columns = ['this_file_name','sentence_order','word_count'], inplace = True)\n",
    "    B = B.fillna(0).as_matrix().transpose()\n",
    "    B = B/B.sum(axis = 0)\n",
    "    # and from the term_sentence_document_matrix we create the term_document_matrix\n",
    "    Y = term_sentence_document_matrix.fillna(0).drop(columns = ['sentence_order','word_count']).groupby('this_file_name').sum().as_matrix().transpose()\n",
    "    # I'll allow 40 topics to be present at any time, so\n",
    "    K = 20\n",
    "    # we must then start by these matrices randomly UsxK: and VdxK\n",
    "    U = np.random.rand(B.shape[1],K)\n",
    "    V = np.random.rand(Y.shape[1],K)\n",
    "    # normalizing each column of U and V\n",
    "    U = U/U.sum(axis = 0)\n",
    "    V = V/V.sum(axis = 1)[:,np.newaxis]\n",
    "\n",
    "    # since we don't know what alfa is supposed to mean, we also randomly initialize it\n",
    "    alfa = np.random.rand(K)\n",
    "    # alfa[:] = 0.5\n",
    "    alfa\n",
    "    diff_u = 100\n",
    "    diff_50 = 10\n",
    "    U_50 = 1000000\n",
    "    it = 0\n",
    "    while(diff_50 > 0.001):\n",
    "        it += 1\n",
    "        old_U = U\n",
    "        U = eq_3(U,B,V,Y)\n",
    "        V = eq_4(U,B,V,Y,alfa)\n",
    "        diff_u = np.abs(U-old_U).sum()\n",
    "        if(it%100 == 0):\n",
    "            diff_50 = np.max(np.abs(U_50 - U).sum(axis = 1))\n",
    "            U_50 = U\n",
    "        if(it%2000 == 0):\n",
    "            print(it,diff_50)\n",
    "    sentence_bank_file = '/home/joao/Thesis/sentence_bank/'+this_file.split('/')[-1]\n",
    "    sentences = pd.read_pickle(sentence_bank_file)\n",
    "    # sentence selection process:\n",
    "    # most likely topic to represent a document:\n",
    "    probabilities = pd.Series(V.flatten())\n",
    "    probabilities = probabilities.sort_values(ascending = False)\n",
    "    topics = []\n",
    "    for i in probabilities.unique():\n",
    "        topics.extend(np.where(V == i)[1])\n",
    "    U_df = pd.DataFrame(U)\n",
    "    selected_sentences_index = []\n",
    "    selected_sentences = []\n",
    "    total_words = 0\n",
    "    for topic in topics:\n",
    "        subset = U_df.iloc[:,topic].copy()\n",
    "        subset.sort_values(ascending = False, inplace = True)\n",
    "        subset = subset[~subset.index.isin(selected_sentences_index)]\n",
    "        selected_sentence = sentences.sentence[subset.index[0]]\n",
    "        total_words += len(selected_sentence.split(' '))\n",
    "        if(total_words < 400):\n",
    "            selected_sentences_index.append(subset.index[0])\n",
    "            selected_sentences.append(selected_sentence)\n",
    "        else:\n",
    "            break\n",
    "    abstract = ''\n",
    "    for i in selected_sentences:\n",
    "        abstract += i\n",
    "    return abstract,this_file.split('/')[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = []\n",
    "\n",
    "result = Parallel(n_jobs = -1, verbose = 11)(delayed(get_u_v_matrices)(i)for i in ts_matrix_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU times: user 281 ms, sys: 42.6 ms, total: 324 ms\n",
    "Wall time: 22min 1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "abstracts = []\n",
    "for i in result:\n",
    "    abstracts.append(i[0])\n",
    "    filenames.append(i[1])\n",
    "results_df = pd.DataFrame({'filename':filenames,'abstracts':abstracts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle('/home/joao/Thesis/BSTM/results.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapting it and running for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from scipy.special import digamma, gamma\n",
    "from scipy.stats import dirichlet\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then begin the iterative procedure:\n",
    "def eq_3(U,B,V,Y):\n",
    "    denominator = np.matmul(np.matmul(B,U),V.transpose())\n",
    "    mid_term = Y/denominator\n",
    "    tmp  = np.dot(np.dot(B.transpose(),mid_term),V)*U\n",
    "    new_U = tmp/tmp.sum(axis = 0)\n",
    "    return new_U\n",
    "def eq_4(U,B,V,Y,alfa):\n",
    "    denominator = np.matmul(np.matmul(B,U),V.transpose())\n",
    "    mid_term  = (Y/denominator).transpose()\n",
    "    X = np.dot(mid_term,np.matmul(B,U))*V\n",
    "    new_V  = dirichlet_adjustment(X,alfa)\n",
    "    return new_V\n",
    "def dirichlet_adjustment(X,alfa):\n",
    "    #pre_creating_the_result_matrix:\n",
    "    # creating the auxiliary list:\n",
    "    X_alfa = X + np.repeat(np.reshape(alfa,(1,alfa.shape[0])), X.shape[0],axis = 0)\n",
    "    sums = np.repeat(np.reshape(X_alfa.sum(axis = 1),(X_alfa.shape[0],1)),X_alfa.shape[1],axis = 1)\n",
    "    new_V = np.exp(digamma(X_alfa) - digamma(sums))\n",
    "    new_V = new_V/new_V.sum(axis = 1)[:,np.newaxis]\n",
    "    return new_V\n",
    "\n",
    "def get_B(alfa):\n",
    "    return(np.prod(gamma(alfa))/gamma(np.sum(alfa)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrix_list = sorted(glob.glob('/home/joao/Thesis/test_set/ts_matrices/*.p'))\n",
    "\n",
    "def get_u_v_matrices(this_file):\n",
    "# we start by loading an example term_sentence matrix\n",
    "    term_sentence_document_matrix = pd.read_pickle(this_file)\n",
    "    #from it we create the term_document matrix and the term-sentence matrix\n",
    "    B = term_sentence_document_matrix.copy()\n",
    "    B_keys = B[['this_file_name','sentence_order','word_count']]\n",
    "    B.drop(columns = ['this_file_name','sentence_order','word_count'], inplace = True)\n",
    "    B = B.fillna(0).as_matrix().transpose()\n",
    "    B = B/B.sum(axis = 0)\n",
    "    # and from the term_sentence_document_matrix we create the term_document_matrix\n",
    "    Y = term_sentence_document_matrix.fillna(0).drop(columns = ['sentence_order','word_count']).groupby('this_file_name').sum().as_matrix().transpose()\n",
    "    # I'll allow 40 topics to be present at any time, so\n",
    "    K = 20\n",
    "    # we must then start by these matrices randomly UsxK: and VdxK\n",
    "    U = np.random.rand(B.shape[1],K)\n",
    "    V = np.random.rand(Y.shape[1],K)\n",
    "    # normalizing each column of U and V\n",
    "    U = U/U.sum(axis = 0)\n",
    "    V = V/V.sum(axis = 1)[:,np.newaxis]\n",
    "\n",
    "    # since we don't know what alfa is supposed to mean, we also randomly initialize it\n",
    "    alfa = np.random.rand(K)\n",
    "    # alfa[:] = 0.5\n",
    "    alfa\n",
    "    diff_u = 100\n",
    "    diff_50 = 10\n",
    "    U_50 = 1000000\n",
    "    it = 0\n",
    "    while(diff_50 > 0.001):\n",
    "        it += 1\n",
    "        old_U = U\n",
    "        U = eq_3(U,B,V,Y)\n",
    "        V = eq_4(U,B,V,Y,alfa)\n",
    "        diff_u = np.abs(U-old_U).sum()\n",
    "        if(it%100 == 0):\n",
    "            diff_50 = np.max(np.abs(U_50 - U).sum(axis = 1))\n",
    "            U_50 = U\n",
    "        if(it%2000 == 0):\n",
    "            print(it,diff_50)\n",
    "    sentence_bank_file = '/home/joao/Thesis/test_set/sentence_banks/'+this_file.split('/')[-1]\n",
    "    sentences = pd.read_pickle(sentence_bank_file)\n",
    "    # sentence selection process:\n",
    "    # most likely topic to represent a document:\n",
    "    probabilities = pd.Series(V.flatten())\n",
    "    probabilities = probabilities.sort_values(ascending = False)\n",
    "    topics = []\n",
    "    for i in probabilities.unique():\n",
    "        topics.extend(np.where(V == i)[1])\n",
    "    U_df = pd.DataFrame(U)\n",
    "    selected_sentences_index = []\n",
    "    selected_sentences = []\n",
    "    total_words = 0\n",
    "    for topic in topics:\n",
    "        subset = U_df.iloc[:,topic].copy()\n",
    "        subset.sort_values(ascending = False, inplace = True)\n",
    "        subset = subset[~subset.index.isin(selected_sentences_index)]\n",
    "        selected_sentence = sentences.sentence[subset.index[0]]\n",
    "        total_words += len(selected_sentence.split(' '))\n",
    "        if(total_words < 250):\n",
    "            selected_sentences_index.append(subset.index[0])\n",
    "            selected_sentences.append(selected_sentence)\n",
    "        else:\n",
    "            break\n",
    "    abstract = ''\n",
    "    for i in selected_sentences:\n",
    "        abstract += i\n",
    "    return abstract,this_file.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrix_list = sorted(glob.glob('/home/joao/Thesis/test_set/ts_matrices/*.p'))\n",
    "\n",
    "result = Parallel(n_jobs = -1, verbose = 11)(delayed(get_u_v_matrices)(i)for i in ts_matrix_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution time: 2minutes 42 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "abstracts = []\n",
    "for i in result:\n",
    "    abstracts.append(i[0])\n",
    "    filenames.append(i[1])\n",
    "results_df = pd.DataFrame({'filename':filenames,'abstracts':abstracts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results_df.abstracts:\n",
    "    print i\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle('/home/joao/Thesis/BSTM/test_results.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge,FilesRouge\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"This function removes all non-ascii characters from text and replaces them with their closest ascii representation\"\"\"\n",
    "    return unidecode(unicode(text, encoding = \"utf-8\"))\n",
    "# we then load all summaries and candidate summaries:\n",
    "\n",
    "total_scores = []\n",
    "scores = []\n",
    "r1 = []\n",
    "r2 = []\n",
    "rl = []\n",
    "\n",
    "for i in tqdm(results_df.index):\n",
    "    ground_truth = '/home/joao/Thesis/test_set/abstracts/ground_truths/'+ results_df.loc[i,'filename'][:-2]+'.txt'\n",
    "    rouge = Rouge()\n",
    "    with open(ground_truth,'rb') as f:\n",
    "        ground_truth = f.read()\n",
    "    ground_truth = remove_non_ascii(ground_truth)\n",
    "    tmp_scores = rouge.get_scores(results_df.loc[i,'abstracts'],ground_truth, avg = True)\n",
    "    r2.append(tmp_scores['rouge-2']['f'])\n",
    "    r1.append(tmp_scores['rouge-1']['f'])\n",
    "    rl.append(tmp_scores['rouge-l']['f'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('r1',np.mean(r1),np.std(r1,ddof = 1))\n",
    "print('r2',np.mean(r2),np.std(r2, ddof = 1))\n",
    "print('rl',np.mean(rl),np.std(rl, ddof = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('r1', 0.3095457283893854, 0.064077106163413)\n",
    "\n",
    "('r2', 0.0969094899709795, 0.052156640692281465)\n",
    "\n",
    "('rl', 0.25649784851398894, 0.06272087587320421)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
