{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We start by importing all the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from gurobipy import *\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We then load an example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrices = pd.Series(sorted(glob.glob('/home/joao/Thesis/ts_matrices_original/*.p')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrix_file = ts_matrices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrix = pd.read_pickle(ts_matrix_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also read the titles dataframe:\n",
    "title_file = '/home/joao/Thesis/titles/'+ ts_matrix_file.split('/')[-1]\n",
    "title = pd.read_pickle(title_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We start by computing the initial hub and authority scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to compare the similarity of the sentences to that of the title\n",
    "# we must put them in the same vector space:\n",
    "#(i.e. for the sake of comparison we temporarily extend both dataframes \n",
    "to the same space)\n",
    "tmp1 = title.drop(columns = ['this_file_name','sentence_order']).copy()\n",
    "tmp2 = ts_matrix.drop(columns = ['this_file_name','sentence_order',\n",
    "                                 'word_count']).copy()\n",
    "connectivity_matrix = ts_matrix.drop(columns = ['this_file_name',\n",
    "                                                'sentence_order',\n",
    "                                                'word_count']) > 0\n",
    "for i in tmp2.columns[~tmp2.columns.isin(tmp1.columns)]:\n",
    "    tmp1[i] = 0\n",
    "for i in tmp1.columns[~tmp1.columns.isin(tmp2.columns)]:\n",
    "    tmp2[i] = 0 \n",
    "    \n",
    "# we must then tag each of the components and consider only nouns\n",
    "result = nltk.pos_tag(tmp1.columns)\n",
    "\n",
    "word = []\n",
    "pos_tag = []\n",
    "for i in result:\n",
    "    word.append(i[0])\n",
    "    pos_tag.append(i[1])\n",
    "pos_tags = pd.DataFrame({'pos_tag':pos_tag,'word':word})\n",
    "useful_tags = pos_tags.word[pos_tags.pos_tag.str.startswith('N')].values\n",
    "tmp1 = tmp1.loc[:,useful_tags]\n",
    "tmp2 = tmp2.loc[:,useful_tags]\n",
    "# we then put them both in the same order:\n",
    "tmp1 = tmp1[sorted(tmp1.columns)]\n",
    "tmp2 = tmp2[sorted(tmp1.columns)]\n",
    "\n",
    "\n",
    "full_title = tmp1.fillna(0).sum(axis = 0)\n",
    "tmp2 = tmp2.fillna(0)\n",
    "# we then calculate the similarities of the hubs (sentences) to the title\n",
    "hub_ranks = 1 + cosine_similarity(tmp2,full_title.reshape(1,-1))\n",
    "# we then calculate the authorities rank\n",
    "authority_ranks = 1 + connectivity_matrix.sum(axis = 0\n",
    "                ) +connectivity_matrix.columns.isin(title.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then apply the hiits algorithm\n",
    "hubs_matrix = np.matmul(connectivity_matrix,\n",
    "                        connectivity_matrix.transpose())\n",
    "authorities_matrix = np.matmul(connectivity_matrix.transpose(),\n",
    "                               connectivity_matrix)\n",
    "\n",
    "max_diff = 10000\n",
    "counter = 1\n",
    "while(max_diff > 0.000000000001 or counter > 100000):\n",
    "    old_hub_ranks = hub_ranks.copy()\n",
    "    hub_ranks = np.matmul(hubs_matrix,hub_ranks)\n",
    "    hub_ranks = hub_ranks/np.linalg.norm(hub_ranks)\n",
    "    old_authority_ranks = authority_ranks.copy()\n",
    "    authority_ranks= np.matmul(authorities_matrix,authority_ranks)\n",
    "    authority_ranks = authority_ranks/np.linalg.norm(authority_ranks)\n",
    "    auth_diff = np.linalg.norm(authority_ranks - old_authority_ranks)\n",
    "    hub_diff = np.linalg.norm(hub_ranks - old_hub_ranks)\n",
    "    max_diff = max(hub_diff,auth_diff)\n",
    "    counter +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we then build the projection of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = np.zeros((connectivity_matrix.shape[0],connectivity_matrix.shape[0]))\n",
    "\n",
    "for i in tqdm(range(connectivity_matrix.shape[0])):\n",
    "    for j in range(i+1,connectivity_matrix.shape[0]):\n",
    "        projection[i,j] = np.logical_and(\n",
    "            connectivity_matrix.loc[i,:],connectivity_matrix.loc[j,:]\n",
    "        ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdegrees = (projection>0).sum(axis = 1)\n",
    "position = range(ts_matrix.shape[0])\n",
    "position = np.array(sorted(position, reverse = True)).astype(float)\n",
    "outdegrees = np.divide(outdegrees,position+1)\n",
    "outdegrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then declare the optimization model: \n",
    "\n",
    "summarizer = Model('summarizer')\n",
    "\n",
    "# we create a list of all sentences:\n",
    "var_names = 'sentence_' + connectivity_matrix.index.astype(str)\n",
    "portvars_x = [summarizer.addVar(vtype = \"B\",\n",
    "                                name = symb) for symb in var_names]\n",
    "portvars_x = pd.Series(portvars_x, index = var_names)\n",
    "portfolio_x = pd.DataFrame({'Variables':portvars_x})\n",
    "entities = connectivity_matrix.columns\n",
    "portvars_y = [summarizer.addVar(vtype = \"B\",\n",
    "                                name = symb) for symb in entities]\n",
    "portvars_y = pd.Series(portvars_y, index = entities)\n",
    "portfolio_y = pd.DataFrame({'Variables':portvars_y})\n",
    "\n",
    "summarizer.update()\n",
    "\n",
    "length = portvars_x.dot(ts_matrix.word_count.values)\n",
    "# we create a similar connectivity_matrix and rename its axis:\n",
    "new_conn = connectivity_matrix.copy()\n",
    "new_conn.index = portvars_x.index\n",
    "\n",
    "# adding the additional consistency contraints on y:\n",
    "for i in new_conn.index:\n",
    "    this_line = new_conn.loc[i,:]\n",
    "    total_entities = (this_line >0).sum()\n",
    "    local_entities = this_line[this_line > 0].index\n",
    "    temp_sum = portvars_y[local_entities].sum()\n",
    "    summarizer.addConstr(temp_sum >= total_entities*portvars_x[i])\n",
    "#adding the additional consistency constraints on x\n",
    "for i in new_conn.columns:\n",
    "    this_column = new_conn.loc[:,i]\n",
    "    total_sentences = this_column.index[this_column > 0]\n",
    "    temp_sum = portvars_x[total_sentences].sum()\n",
    "    summarizer.addConstr(temp_sum >= portvars_y[i])\n",
    "word_limit = 400\n",
    "\n",
    "# adding the final constraint on model size \n",
    "summarizer.addConstr(length <= 400)\n",
    "\n",
    "# defining the objective function:\n",
    "coverage = portvars_x.dot(hub_ranks)\n",
    "coherence = portvars_x.dot(outdegrees)\n",
    "diversity = portvars_y.sum()/portvars_y.shape[0]\n",
    "\n",
    "summarizer.setObjective((coverage + coherence + diversity)[0],\n",
    "                        GRB.MAXIMIZE)\n",
    "\n",
    "summarizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summarizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we process the answers:\n",
    "selected_sentences = []\n",
    "for i in portvars_x:\n",
    "    selected_sentences.append(i.getAttr('x'))\n",
    "selected_sentences = pd.Series(selected_sentences)\n",
    "\n",
    "# we then load the sentence_banks to check the results\n",
    "sentence_bank_file = ('/home/joao/Thesis/sentence_bank/'+ \n",
    "                        ts_matrix_file.split('/')[-1])\n",
    "\n",
    "sentence_bank = pd.read_pickle(sentence_bank_file)\n",
    "\n",
    "abstract = sentence_bank.loc[selected_sentences > 0,'sentence']\n",
    "\n",
    "final_abstract = ''\n",
    "for i in abstract:\n",
    "    final_abstract += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's score it! \n",
    "from rouge import Rouge,FilesRouge\n",
    "\n",
    "# we then load all summaries and candidate summaries:\n",
    "true_summaries = pd.Series(sorted(glob.glob(\n",
    "    '/home/joao/Thesis/simplified_abstracts/*')))\n",
    "\n",
    "total_scores = []\n",
    "this_file = ts_matrix_file.split('/')[-1]\n",
    "this_file_num = this_file.split('.')[-2][1:4]\n",
    "ground_truths = true_summaries[true_summaries.str[-7:-4] == \n",
    "                               this_file_num]\n",
    "scores = []\n",
    "rouge = Rouge()\n",
    "auto_summary = final_abstract\n",
    "for j in ground_truths:\n",
    "    with open(j,'rb') as f:\n",
    "        ground_truth = f.read()\n",
    "        tmp_scores = rouge.get_scores(auto_summary,ground_truth,\n",
    "                                      avg = True)\n",
    "    scores.append(tmp_scores['rouge-2']['p'])\n",
    "total_scores.append(np.mean(scores))\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now turning it into a function and running it for the entire database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## relevant imports \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from gurobipy import *\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(ts_matrix_file):\n",
    "    ts_matrix = pd.read_pickle(ts_matrix_file)\n",
    "    # we also read the titles dataframe:\n",
    "    title_file = '/home/joao/Thesis/titles/'+ ts_matrix_file.split(\n",
    "        '/')[-1]\n",
    "    title = pd.read_pickle(title_file)\n",
    "    #in order to compare the similarity of the sentences to \n",
    "    #that of the title\n",
    "    # we must put them in the same vector space:\n",
    "    #(i.e. for the sake of comparison we temporarily extend \n",
    "    #both dataframes to the same space)\n",
    "    tmp1 = title.drop(columns = ['this_file_name',\n",
    "                                 'sentence_order']).copy()\n",
    "    tmp2 = ts_matrix.drop(columns = ['this_file_name',\n",
    "                                     'sentence_order','word_count'\n",
    "                                    ]).copy()\n",
    "    connectivity_matrix = ts_matrix.drop(columns = ['this_file_name',\n",
    "                                                    'sentence_order',\n",
    "                                                    'word_count']).copy() > 0\n",
    "    for i in tmp2.columns[~tmp2.columns.isin(tmp1.columns)]:\n",
    "        tmp1[i] = 0\n",
    "    for i in tmp1.columns[~tmp1.columns.isin(tmp2.columns)]:\n",
    "        tmp2[i] = 0 \n",
    "\n",
    "#     we must then tag each of the components and consider only nouns\n",
    "    result = nltk.pos_tag(tmp1.columns)\n",
    "\n",
    "    word = []\n",
    "    pos_tag = []\n",
    "    for i in result:\n",
    "        word.append(i[0])\n",
    "        pos_tag.append(i[1])\n",
    "    pos_tags = pd.DataFrame({'pos_tag':pos_tag,'word':word})\n",
    "    useful_tags = pos_tags.word[pos_tags.pos_tag.str.startswith('N')].values\n",
    "    # we then remove all empty rows\n",
    "    connectivity_matrix = connectivity_matrix.loc[:,connectivity_matrix.columns[connectivity_matrix.columns.isin(useful_tags)]] >0\n",
    "    indices_to_drop = connectivity_matrix.index[connectivity_matrix.sum(axis = 1)==0]\n",
    "    connectivity_matrix = connectivity_matrix.drop(index=  indices_to_drop) >0\n",
    "    original_indices = connectivity_matrix.index\n",
    "    connectivity_matrix.reset_index(inplace = True,drop = True)\n",
    "    tmp1 = tmp1.loc[:,useful_tags]\n",
    "    tmp2 = tmp2.loc[:,useful_tags]\n",
    "    print(tmp2.index)\n",
    "    tmp2.drop(index= indices_to_drop, inplace = True)\n",
    "    tmp2.reset_index(inplace = True, drop = True)\n",
    "    ts_matrix.drop(index = indices_to_drop, inplace = True)\n",
    "    ts_matrix.reset_index(inplace = True, drop = True)\n",
    "    # we then put them both in the same order:\n",
    "\n",
    "    tmp1 = tmp1[sorted(tmp1.columns)]\n",
    "    tmp2 = tmp2[sorted(tmp1.columns)]\n",
    "\n",
    "    full_title = tmp1.fillna(0).sum(axis = 0)\n",
    "    tmp2 = tmp2.fillna(0)\n",
    "    # we then calculate the similarities of the hubs (sentences) to the title\n",
    "    hub_ranks = 1 + cosine_similarity(tmp2,full_title.reshape(1,-1))\n",
    "    # we then calculate the authorities rank\n",
    "    authority_ranks = 1 + connectivity_matrix.sum(\n",
    "        axis = 0) +connectivity_matrix.columns.isin(title.columns)\n",
    "    # we then apply the hiits algorithm\n",
    "    hubs_matrix = np.matmul(connectivity_matrix,connectivity_matrix.transpose())\n",
    "    authorities_matrix = np.matmul(connectivity_matrix.transpose(),connectivity_matrix)\n",
    "\n",
    "    max_diff = 10000\n",
    "    counter = 1\n",
    "    while(max_diff > 0.000000000001 or counter > 100000):\n",
    "        old_hub_ranks = hub_ranks.copy()\n",
    "        hub_ranks = np.matmul(hubs_matrix,hub_ranks)\n",
    "        hub_ranks = hub_ranks/np.linalg.norm(hub_ranks)\n",
    "        old_authority_ranks = authority_ranks.copy()\n",
    "        authority_ranks= np.matmul(authorities_matrix,authority_ranks)\n",
    "        authority_ranks = authority_ranks/np.linalg.norm(authority_ranks)\n",
    "        auth_diff = np.linalg.norm(authority_ranks - old_authority_ranks)\n",
    "        hub_diff = np.linalg.norm(hub_ranks - old_hub_ranks)\n",
    "        max_diff = max(hub_diff,auth_diff)\n",
    "        counter +=1\n",
    "    projection = np.zeros((connectivity_matrix.shape[0],\n",
    "                           connectivity_matrix.shape[0]))\n",
    "\n",
    "    for i in tqdm(range(connectivity_matrix.shape[0])):\n",
    "        for j in range(i+1,connectivity_matrix.shape[0]):\n",
    "            projection[i,j] = np.logical_and(connectivity_matrix.loc[i,:],connectivity_matrix.loc[j,:]).sum()\n",
    "\n",
    "    outdegrees = (projection>0).sum(axis = 1)\n",
    "    position = original_indices\n",
    "    position = np.array(sorted(position, reverse = True)).astype(float)\n",
    "    outdegrees = np.divide(outdegrees,position+1)\n",
    "    # we then declare the optimization model: \n",
    "\n",
    "    summarizer = Model('summarizer')\n",
    "\n",
    "    # we create a list of all sentences:\n",
    "    var_names = 'sentence_' + connectivity_matrix.index.astype(str)\n",
    "    portvars_x = [summarizer.addVar(vtype = \"B\",\n",
    "                                    name = symb) for symb in var_names]\n",
    "    portvars_x = pd.Series(portvars_x, index = var_names)\n",
    "    portfolio_x = pd.DataFrame({'Variables':portvars_x})\n",
    "    entities = connectivity_matrix.columns\n",
    "    portvars_y = [summarizer.addVar(vtype = \"B\",\n",
    "                                    name = symb) for symb in entities]\n",
    "    portvars_y = pd.Series(portvars_y, index = entities)\n",
    "    portfolio_y = pd.DataFrame({'Variables':portvars_y})\n",
    "\n",
    "    summarizer.update()\n",
    "\n",
    "    length = portvars_x.dot(ts_matrix.word_count.values)\n",
    "    # we create a similar connectivity_matrix and rename its axis:\n",
    "    new_conn = connectivity_matrix.copy()\n",
    "    new_conn.index = portvars_x.index\n",
    "\n",
    "    # adding the additional consistency contraints on y:\n",
    "    for i in new_conn.index:\n",
    "        this_line = new_conn.loc[i,:]\n",
    "        total_entities = (this_line >0).sum()\n",
    "        local_entities = this_line[this_line > 0].index\n",
    "        if(local_entities.size != 0):\n",
    "            temp_sum = portvars_y[local_entities].sum()\n",
    "            summarizer.addConstr(temp_sum >= \n",
    "                                 total_entities*portvars_x[i])\n",
    "    #adding the additional consistency constraints on x\n",
    "    for i in new_conn.columns:\n",
    "        this_column = new_conn.loc[:,i]\n",
    "        total_sentences = this_column.index[this_column > 0]\n",
    "        temp_sum = portvars_x[total_sentences].sum()\n",
    "        summarizer.addConstr(temp_sum >= portvars_y[i])\n",
    "    word_limit = 400\n",
    "\n",
    "    # adding the final constraint on model size \n",
    "    summarizer.addConstr(length <= word_limit)\n",
    "\n",
    "    # defining the objective function:\n",
    "    coverage = portvars_x.dot(hub_ranks)\n",
    "    coherence = portvars_x.dot(outdegrees)\n",
    "    diversity = portvars_y.sum()/portvars_y.shape[0]\n",
    "\n",
    "    summarizer.setObjective((coverage + coherence + diversity)[0],\n",
    "                            GRB.MAXIMIZE)\n",
    "\n",
    "    summarizer.update()\n",
    "    \n",
    "    #execute the optimization\n",
    "    summarizer.optimize()\n",
    "    \n",
    "    # we process the answers:\n",
    "    selected_sentences = []\n",
    "    for i in portvars_x:\n",
    "        selected_sentences.append(i.getAttr('x'))\n",
    "    selected_sentences = pd.Series(selected_sentences)\n",
    "\n",
    "    # we then load the sentence_banks to check the results\n",
    "    sentence_bank_file = ('/home/joao/Thesis/sentence_bank/'+ \n",
    "                          ts_matrix_file.split('/')[-1])\n",
    "    sentence_bank = pd.read_pickle(sentence_bank_file)\n",
    "    sentence_bank.drop(index=  indices_to_drop, inplace = True)\n",
    "    sentence_bank.reset_index(inplace = True, drop = True)\n",
    "    abstract = sentence_bank.loc[selected_sentences > 0,'sentence']\n",
    "\n",
    "    final_abstract = ''\n",
    "    for i in abstract:\n",
    "        final_abstract += i\n",
    "    return(ts_matrix_file,final_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrices = pd.Series(sorted(glob.glob('/home/joao/Thesis/ts_matrices_original/*.p')))\n",
    "final_results = Parallel(n_jobs = -1, verbose = 11)(delayed(\n",
    "    summarize_text)(ts_matrix_file)for ts_matrix_file in ts_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution time : 6 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "abstracts = []\n",
    "for i in final_results:\n",
    "    files.append(i[0])\n",
    "    abstracts.append(i[1])\n",
    "final_df = pd.DataFrame({'filenames':files,'abstracts':abstracts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle('/home/joao/Thesis/sum_coh/train_results.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how we did! \n",
    "true_summaries = pd.Series(sorted(glob.glob(\n",
    "    '/home/joao/Thesis/simplified_abstracts/*')))\n",
    "total_scores = []\n",
    "for i in final_df.index:\n",
    "    this_file = final_df.loc[i,'filenames'].split('/')[-1]\n",
    "    this_file_num = this_file.split('.')[-2][1:4]\n",
    "    ground_truths = true_summaries[true_summaries.str[-7:-4] == \n",
    "                                   this_file_num]\n",
    "    scores = []\n",
    "    rouge = Rouge()\n",
    "    auto_summary = final_df.loc[i,'abstracts']\n",
    "    for j in ground_truths:\n",
    "        with open(j,'rb') as f:\n",
    "            ground_truth = f.read()\n",
    "            tmp_scores = rouge.get_scores(auto_summary,\n",
    "                                          ground_truth,avg = True)\n",
    "        scores.append(tmp_scores['rouge-2']['p'])\n",
    "    total_scores.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_series = pd.Series(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now adapting the function to run for the articles database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## relevant imports \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from gurobipy import *\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "def summarize_text(ts_matrix_file):\n",
    "    ts_matrix = pd.read_pickle(ts_matrix_file)\n",
    "    # we also read the titles dataframe:\n",
    "    title_file = ('/home/joao/Thesis/test_set/titles/'+ \n",
    "                  ts_matrix_file.split('/')[-1])\n",
    "    title = pd.read_pickle(title_file)\n",
    "    #in order to compare the similarity of the sentences to that of the title\n",
    "    # we must put them in the same vector space:\n",
    "    #(i.e. for the sake of comparison we temporarily extend both dataframes to the same space)\n",
    "    tmp1 = title.drop(columns = ['this_file_name',\n",
    "                                 'sentence_order']).copy()\n",
    "    tmp2 = ts_matrix.drop(columns = ['this_file_name',\n",
    "                                     'sentence_order','word_count'\n",
    "                                    ]).copy()\n",
    "    connectivity_matrix = ts_matrix.drop(columns = ['this_file_name',\n",
    "                                                    'sentence_order',\n",
    "                                                    'word_count']).copy() > 0\n",
    "    for i in tmp2.columns[~tmp2.columns.isin(tmp1.columns)]:\n",
    "        tmp1[i] = 0\n",
    "    for i in tmp1.columns[~tmp1.columns.isin(tmp2.columns)]:\n",
    "        tmp2[i] = 0 \n",
    "\n",
    "#     we must then tag each of the components and consider only nouns\n",
    "    result = nltk.pos_tag(tmp1.columns)\n",
    "\n",
    "    word = []\n",
    "    pos_tag = []\n",
    "    for i in result:\n",
    "        word.append(i[0])\n",
    "        pos_tag.append(i[1])\n",
    "    pos_tags = pd.DataFrame({'pos_tag':pos_tag,'word':word})\n",
    "    useful_tags = pos_tags.word[pos_tags.pos_tag.str.startswith('N')].values\n",
    "    # we then remove all empty rows\n",
    "    connectivity_matrix = connectivity_matrix.loc[:,connectivity_matrix.columns[connectivity_matrix.columns.isin(useful_tags)]] >0\n",
    "    indices_to_drop = connectivity_matrix.index[connectivity_matrix.sum(axis = 1)==0]\n",
    "    connectivity_matrix = connectivity_matrix.drop(index=  indices_to_drop) >0\n",
    "    original_indices = connectivity_matrix.index\n",
    "    connectivity_matrix.reset_index(inplace = True,drop = True)\n",
    "    tmp1 = tmp1.loc[:,useful_tags]\n",
    "    tmp2 = tmp2.loc[:,useful_tags]\n",
    "    print(tmp2.index)\n",
    "    tmp2.drop(index= indices_to_drop, inplace = True)\n",
    "    tmp2.reset_index(inplace = True, drop = True)\n",
    "    ts_matrix.drop(index = indices_to_drop, inplace = True)\n",
    "    ts_matrix.reset_index(inplace = True, drop = True)\n",
    "    # we then put them both in the same order:\n",
    "\n",
    "    tmp1 = tmp1[sorted(tmp1.columns)]\n",
    "    tmp2 = tmp2[sorted(tmp1.columns)]\n",
    "\n",
    "    full_title = tmp1.fillna(0).sum(axis = 0)\n",
    "    tmp2 = tmp2.fillna(0)\n",
    "    # we then calculate the similarities of the hubs (sentences) to the title\n",
    "    hub_ranks = 1 + cosine_similarity(tmp2,full_title.reshape(1,-1))\n",
    "    # we then calculate the authorities rank\n",
    "    authority_ranks = 1 + connectivity_matrix.sum(\n",
    "        axis = 0) +connectivity_matrix.columns.isin(title.columns)\n",
    "    # we then apply the hiits algorithm\n",
    "    hubs_matrix = np.matmul(connectivity_matrix,connectivity_matrix.transpose())\n",
    "    authorities_matrix = np.matmul(connectivity_matrix.transpose(),connectivity_matrix)\n",
    "\n",
    "    max_diff = 10000\n",
    "    counter = 1\n",
    "    while(max_diff > 0.000000000001 or counter > 100000):\n",
    "        old_hub_ranks = hub_ranks.copy()\n",
    "        hub_ranks = np.matmul(hubs_matrix,hub_ranks)\n",
    "        hub_ranks = hub_ranks/np.linalg.norm(hub_ranks)\n",
    "        old_authority_ranks = authority_ranks.copy()\n",
    "        authority_ranks= np.matmul(authorities_matrix,authority_ranks)\n",
    "        authority_ranks = authority_ranks/np.linalg.norm(authority_ranks)\n",
    "        auth_diff = np.linalg.norm(authority_ranks - old_authority_ranks)\n",
    "        hub_diff = np.linalg.norm(hub_ranks - old_hub_ranks)\n",
    "        max_diff = max(hub_diff,auth_diff)\n",
    "        counter +=1\n",
    "    projection = np.zeros((connectivity_matrix.shape[0],\n",
    "                           connectivity_matrix.shape[0]))\n",
    "\n",
    "    for i in tqdm(range(connectivity_matrix.shape[0])):\n",
    "        for j in range(i+1,connectivity_matrix.shape[0]):\n",
    "            projection[i,j] = np.logical_and(connectivity_matrix.loc[i,:],connectivity_matrix.loc[j,:]).sum()\n",
    "\n",
    "    outdegrees = (projection>0).sum(axis = 1)\n",
    "    position = original_indices\n",
    "    position = np.array(sorted(position, reverse = True)).astype(float)\n",
    "    outdegrees = np.divide(outdegrees,position+1)\n",
    "    # we then declare the optimization model: \n",
    "\n",
    "    summarizer = Model('summarizer')\n",
    "\n",
    "    # we create a list of all sentences:\n",
    "    var_names = 'sentence_' + connectivity_matrix.index.astype(str)\n",
    "    portvars_x = [summarizer.addVar(vtype = \"B\",\n",
    "                                    name = symb) for symb in var_names]\n",
    "    portvars_x = pd.Series(portvars_x, index = var_names)\n",
    "    portfolio_x = pd.DataFrame({'Variables':portvars_x})\n",
    "    entities = connectivity_matrix.columns\n",
    "    portvars_y = [summarizer.addVar(vtype = \"B\",\n",
    "                                    name = symb) for symb in entities]\n",
    "    portvars_y = pd.Series(portvars_y, index = entities)\n",
    "    portfolio_y = pd.DataFrame({'Variables':portvars_y})\n",
    "\n",
    "    summarizer.update()\n",
    "\n",
    "    length = portvars_x.dot(ts_matrix.word_count.values)\n",
    "    # we create a similar connectivity_matrix and rename its axis:\n",
    "    new_conn = connectivity_matrix.copy()\n",
    "    new_conn.index = portvars_x.index\n",
    "\n",
    "    # adding the additional consistency contraints on y:\n",
    "    for i in new_conn.index:\n",
    "        this_line = new_conn.loc[i,:]\n",
    "        total_entities = (this_line >0).sum()\n",
    "        local_entities = this_line[this_line > 0].index\n",
    "        if(local_entities.size != 0):\n",
    "            temp_sum = portvars_y[local_entities].sum()\n",
    "            summarizer.addConstr(temp_sum >= \n",
    "                                 total_entities*portvars_x[i])\n",
    "    #adding the additional consistency constraints on x\n",
    "    for i in new_conn.columns:\n",
    "        this_column = new_conn.loc[:,i]\n",
    "        total_sentences = this_column.index[this_column > 0]\n",
    "        temp_sum = portvars_x[total_sentences].sum()\n",
    "        summarizer.addConstr(temp_sum >= portvars_y[i])\n",
    "    word_limit = 250\n",
    "\n",
    "    # adding the final constraint on model size \n",
    "    summarizer.addConstr(length <= word_limit)\n",
    "\n",
    "    # defining the objective function:\n",
    "    coverage = portvars_x.dot(hub_ranks)\n",
    "    coherence = portvars_x.dot(outdegrees)\n",
    "    diversity = portvars_y.sum()/portvars_y.shape[0]\n",
    "\n",
    "    summarizer.setObjective((coverage + coherence + diversity)[0],\n",
    "                            GRB.MAXIMIZE)\n",
    "\n",
    "    summarizer.update()\n",
    "    \n",
    "    #execute the optimization\n",
    "    summarizer.optimize()\n",
    "    \n",
    "    # we process the answers:\n",
    "    selected_sentences = []\n",
    "    for i in portvars_x:\n",
    "        selected_sentences.append(i.getAttr('x'))\n",
    "    selected_sentences = pd.Series(selected_sentences)\n",
    "\n",
    "    # we then load the sentence_banks to check the results\n",
    "    sentence_bank_file = ('/home/joao/Thesis/test_set/sentence_banks/'+ \n",
    "                          ts_matrix_file.split('/')[-1])\n",
    "\n",
    "    sentence_bank = pd.read_pickle(sentence_bank_file)\n",
    "    sentence_bank.drop(index=  indices_to_drop, inplace = True)\n",
    "    sentence_bank.reset_index(inplace = True, drop = True)\n",
    "    abstract = sentence_bank.loc[selected_sentences > 0,'sentence']\n",
    "\n",
    "    final_abstract = ''\n",
    "    for i in abstract:\n",
    "        final_abstract += i\n",
    "    return(ts_matrix_file,final_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrices = pd.Series(sorted(glob.glob(\n",
    "    '/home/joao/Thesis/test_set/ts_matrices/*.p')))\n",
    "final_results = Parallel(n_jobs = -1, verbose = 11)(delayed(\n",
    "    summarize_text)(ts_matrix_file)for ts_matrix_file in ts_matrices)\n",
    "\n",
    "files = []\n",
    "abstracts = []\n",
    "for i in final_results:\n",
    "    files.append(i[0])\n",
    "    abstracts.append(i[1])\n",
    "final_df = pd.DataFrame({'filenames':files,'abstracts':abstracts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution time: 1 minute 40 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle('/home/joao/Thesis/sum_coh/test_set_results.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:03<00:00, 21.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge,FilesRouge\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "results_df = pd.read_pickle('/home/joao/Thesis/sum_coh/test_set_results.p')\n",
    "results_df.filenames = results_df.filenames.str.split('/', expand = True)[6]\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"This function removes all non-ascii characters from\n",
    "    text and replaces them with their closest ascii representation\"\"\"\n",
    "    return unidecode(unicode(text, encoding = \"utf-8\"))\n",
    "# we then load all summaries and candidate summaries:\n",
    "\n",
    "total_scores = []\n",
    "scores = []\n",
    "r1 = []\n",
    "r2 = []\n",
    "rl = []\n",
    "\n",
    "for i in tqdm(results_df.index):\n",
    "    ground_truth = ('/home/joao/Thesis/test_set/abstracts/ground_truths/'+ \n",
    "                    results_df.loc[i,'filenames'][:-2]+'.txt')\n",
    "    rouge = Rouge()\n",
    "    with open(ground_truth,'rb') as f:\n",
    "        ground_truth = f.read()\n",
    "    ground_truth = remove_non_ascii(ground_truth)\n",
    "    tmp_scores = rouge.get_scores(results_df.loc[i,'abstracts'],\n",
    "                                  ground_truth, avg = True)\n",
    "    r2.append(tmp_scores['rouge-2']['f'])\n",
    "    r1.append(tmp_scores['rouge-1']['f'])\n",
    "    rl.append(tmp_scores['rouge-l']['f'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('r1',np.mean(r1),np.std(r1,ddof = 1))\n",
    "print('r2',np.mean(r2),np.std(r2, ddof = 1))\n",
    "print('rl',np.mean(rl),np.std(rl, ddof = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('r1', 0.34164433082277473, 0.06061530353507446)\n",
    "\n",
    "('r2', 0.11464533396916964, 0.05274832618517848)\n",
    "\n",
    "('rl', 0.3012795744110309, 0.06235428378544586)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
