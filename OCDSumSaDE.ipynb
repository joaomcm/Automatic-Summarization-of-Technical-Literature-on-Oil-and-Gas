{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing an example of a term_sentence matrix\n",
    "ts_matrix_files = sorted(glob.glob('/home/joao/Thesis/ts_matrices_original/*.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrix_file = ts_matrix_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_matrix = pd.read_pickle(ts_matrix_file)\n",
    "ts_keys = ts_matrix[['this_file_name','sentence_order','word_count']]\n",
    "ts_matrix = ts_matrix.drop(columns = ['this_file_name','sentence_order','word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we must then build the ts-isf matrix:\n",
    "# we start by building the isf vector\n",
    "isf = np.log(float(ts_matrix.shape[0])/(ts_matrix >0).sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then build the ts_isf matrix:\n",
    "ts_isf = (ts_matrix*isf).fillna(0).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# we then define the similarity between two vectors as the cosine similarity\n",
    "sentence_similarities  = cosine_similarity(ts_isf.mean(axis = 0).reshape(1,-1),ts_isf)\n",
    "full_similarities = cosine_similarity(ts_isf,ts_isf)\n",
    "mean_overall_vector = ts_isf.mean(axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage(X):\n",
    "    if(X.sum() == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        summary = ts_isf[X,:]\n",
    "        mean_summary_vector = summary.mean(axis = 0)\n",
    "        coverage = cosine_similarity(mean_overall_vector.reshape(1,-1),mean_summary_vector.reshape(1,-1))*np.matmul(sentence_similarities,X)\n",
    "    return coverage\n",
    "\n",
    "def get_diversity(X):\n",
    "    if(X.sum() == 0):\n",
    "        return 1000000\n",
    "    # we start by creating a map of which sentences are to be selected in this measure\n",
    "    selection_matrix = np.matmul(X.reshape(-1,1),X.reshape(1,-1))\n",
    "    diversity  = float((np.multiply(full_similarities,selection_matrix)).sum() - X.sum())/2\n",
    "    if(diversity == 0):\n",
    "        return 10000\n",
    "    return diversity\n",
    "\n",
    "def objective_function(X):\n",
    "    a = get_coverage(X)\n",
    "    b = get_diversity(X)\n",
    "    return(float(a)/(b))\n",
    "    \n",
    "def get_FT(iterations,itermax):\n",
    "    return np.exp(-2*float(iterations)/itermax)\n",
    "\n",
    "def clip_population(population):\n",
    "    population[population < umin] = 2*umin - population[population < umin]\n",
    "    population[population > umax] = 2*umax - population[population > umax]\n",
    "    return population\n",
    "\n",
    "def get_crossover_rate(actual_scores):\n",
    "    RD = (max_score - actual_scores)/float((max_score - min_score))\n",
    "    hiptan = np.tanh(2*RD)\n",
    "    CR = 2*hiptan/(1+hiptan)\n",
    "    return CR\n",
    "\n",
    "def binarize(population):\n",
    "    random_draw = np.random.rand(population.shape[0],population.shape[1])\n",
    "    new_X = (random_draw < expit(population.values)).astype(int)\n",
    "    return new_X\n",
    "\n",
    "def evaluate_summary_candidates(X):\n",
    "    scores = []\n",
    "    for i in range(X.shape[0]): \n",
    "        scores.append(objective_function(X[i,:]))\n",
    "    return pd.Series(scores)\n",
    "\n",
    "def perform_crossover(population):\n",
    "    reference = population.sample(frac = 1, replace = True).reset_index(drop = True)\n",
    "    FT = get_FT(iteration,itermax)\n",
    "    mutated_candidates = initial_population + (1.0-FT)*(global_best_individual - reference) + FT*(best_individual - reference)\n",
    "    mutated_candidates = clip_population(mutated_candidates)\n",
    "    CR = get_crossover_rate(scores)\n",
    "    CR = np.repeat(CR.values,initial_population.shape[1])\n",
    "    CR = CR.reshape((P,-1))\n",
    "    random_draw = pd.DataFrame(np.random.rand(initial_population.shape[0],initial_population.shape[1]))\n",
    "    keepers = (random_draw <= CR).values\n",
    "    K = np.floor(float(initial_population.shape[1])*np.random.rand(initial_population.shape[0])).astype(int)\n",
    "    future_generation = initial_population.copy()\n",
    "    future_generation[keepers] = mutated_candidates[keepers]\n",
    "    for i in future_generation.index.values:\n",
    "        future_generation.iloc[i,K[i]] = mutated_candidates.loc[i,K[i]]\n",
    "    future_generation = clip_population(future_generation)\n",
    "    return future_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now define the whole genetic algorithmframework:\n",
    "\n",
    "P = 100\n",
    "itermax = 1000\n",
    "sentence_limit = 400.0\n",
    "umin = -5\n",
    "umax = 5\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "initial_population = pd.DataFrame(10.0*(np.random.rand(P,ts_isf.shape[0]))-(1-sentence_limit/ts_keys.word_count.sum())*10)\n",
    "initial_population = clip_population(initial_population)\n",
    "# we then discretize the problem\n",
    "X = binarize(initial_population)\n",
    "\n",
    "# we check which of the sentences is possibly valid by checking the word count of the summary\n",
    "invalid_score = (sentence_limit - np.matmul(X,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,))\n",
    "invalid_score = invalid_score/sentence_limit\n",
    "invalid_population = (invalid_score < 0)\n",
    "\n",
    "# we then compute the population scores\n",
    "scores = evaluate_summary_candidates(X)\n",
    "scores[invalid_population] = invalid_score[invalid_population]\n",
    "# find the best and worst individuals\n",
    "max_score = scores.max()\n",
    "min_score = scores.min()\n",
    "best_individual = initial_population.loc[scores[scores.values == scores.max()].index[0],:].values\n",
    "worst_individual = initial_population.loc[scores[scores.values == scores.min()].index[0],:].values\n",
    "# at first, the global best and the best are the same\n",
    "global_best_individual = best_individual\n",
    "global_best_score = max_score\n",
    "\n",
    "next_generation = perform_crossover(initial_population)\n",
    "# print((next_generation>0).sum())\n",
    "X_mutated = binarize(next_generation)\n",
    "# we then compute if the mutated scores are valid or not\n",
    "invalid_score = sentence_limit - np.matmul(X_mutated,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,)\n",
    "invalid_score = invalid_score/sentence_limit\n",
    "invalid_population = (invalid_score < 0)\n",
    "# we then compute the mutated scores\n",
    "mutated_scores = evaluate_summary_candidates(X_mutated)\n",
    "mutated_scores[invalid_population] = invalid_score\n",
    "if(mutated_scores.max() > max_score):\n",
    "    max_score = mutated_scores.max()\n",
    "    best_individual = next_generation.iloc[mutated_scores[mutated_scores == max_score].index[0],:].values\n",
    "    if(max_score > global_best_score):\n",
    "        global_best_score = max_score\n",
    "        global_best_individual = next_generation.loc[mutated_scores[mutated_scores == max_score].index[0],:].values\n",
    "worse = (mutated_scores < scores).values\n",
    "\n",
    "next_generation.loc[worse,:] = initial_population.loc[worse,:] \n",
    "\n",
    "X_mutated[worse,:] = X[worse,:]\n",
    "X = X_mutated\n",
    "worse.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for iteration in range(1,itermax):\n",
    "    # we then repeat the process for each successive iteration:\n",
    "\n",
    "    initial_population = next_generation.copy()\n",
    "    X = binarize(initial_population)\n",
    "    # we then discretize the problem\n",
    "    \n",
    "    # we check which of the sentences is possibly valid by checking the word count of the summary\n",
    "    invalid_score = sentence_limit - np.matmul(X,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,)\n",
    "    invalid_score = invalid_score/sentence_limit\n",
    "    invalid_population = (invalid_score < 0)\n",
    "    #we then binarize the population:\n",
    "    \n",
    "    # we then compute the population scores\n",
    "    scores = evaluate_summary_candidates(X)\n",
    "    scores[invalid_population] = invalid_score[invalid_population]\n",
    "        # find the best and worst individuals\n",
    "    max_score = scores.max()\n",
    "    min_score = scores.min()\n",
    "    best_individual = initial_population.loc[scores[scores.values == scores.max()].index[0],:].values\n",
    "    \n",
    "    if(max_score > global_best_score):\n",
    "        global_best_score = max_score\n",
    "        global_best_individual = best_individual\n",
    "        global_best_X = X[scores[scores.values == scores.max()].index[0]]\n",
    "    next_generation = perform_crossover(initial_population)\n",
    "    X_mutated = binarize(next_generation)\n",
    "    # we then compute if the mutated scores are valid or not\n",
    "    invalid_score = sentence_limit - np.matmul(X_mutated,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,)\n",
    "    invalid_score = invalid_score/sentence_limit\n",
    "    invalid_population = (invalid_score < 0)\n",
    "    # we then compute the mutated scores\n",
    "    mutated_scores = evaluate_summary_candidates(X_mutated)\n",
    "    mutated_scores[invalid_population] = invalid_score\n",
    "    if(mutated_scores.max() > max_score):\n",
    "        max_score = mutated_scores.max()\n",
    "        best_individual = next_generation.iloc[mutated_scores[mutated_scores == max_score].index[0],:].values\n",
    "        if(max_score > global_best_score):\n",
    "            global_best_score = max_score\n",
    "            global_best_individual = next_generation.iloc[mutated_scores[mutated_scores == max_score].index[0],:].values\n",
    "            global_best_X = X_mutated[mutated_scores[mutated_scores.values == mutated_scores.max()].index[0],:]\n",
    "    worse = mutated_scores < scores\n",
    "    next_generation.loc[worse,:] = initial_population.loc[worse,:]\n",
    "    X_mutated[worse,:] = X[worse,:]\n",
    "    X = X_mutated\n",
    "    if(iteration%25 == 0):\n",
    "        print(iteration,mutated_scores.mean(),scores.mean(),scores.max(),worse.sum(),global_best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sorted(glob.glob('/home/joao/Thesis/sentence_bank/*.p'))\n",
    "sentence_bank = pd.read_pickle(a[0])\n",
    "sentence_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentence = sentence_bank.loc[global_best_X == 1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = ''\n",
    "for i in final_sentence.sentence:\n",
    "    abstract += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge,FilesRouge\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_summaries = pd.Series(sorted(glob.glob('/home/joao/Thesis/simplified_abstracts/*')))\n",
    "true_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = []\n",
    "i = ts_matrix_file\n",
    "this_file = i.split('/')[-1]\n",
    "this_file_num = this_file.split('.')[0][1:4]\n",
    "ground_truths = true_summaries[true_summaries.str[-7:-4] == this_file_num]\n",
    "scores = []\n",
    "rouge = Rouge()\n",
    "abstract\n",
    "for j in ground_truths:\n",
    "    with open(j,'rb') as f:\n",
    "        ground_truth = f.read()\n",
    "        tmp_scores = rouge.get_scores(abstract,ground_truth,avg = True)\n",
    "    scores.append(tmp_scores['rouge-2']['p'])\n",
    "total_scores.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# turning it all into a function and running it for the entire base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit\n",
    "from rouge import Rouge,FilesRouge\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# We start by importing an example of a term_sentence matrix\n",
    "ts_matrix_files = sorted(glob.glob('/home/joao/Thesis/ts_matrices_original/*.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class summarizer:\n",
    "    def __init__(self,ts_matrix_file):\n",
    "        self.ts_matrix_file = ts_matrix_file\n",
    "        sentence_bank_file = '/home/joao/Thesis/sentence_bank/' + ts_matrix_file.split('/')[-1]\n",
    "        self.sentence_bank = pd.read_pickle(sentence_bank_file)\n",
    "        self.P = 100\n",
    "        self.itermax = 1000\n",
    "        self.sentence_limit = 400.0\n",
    "        self.umin = -5\n",
    "        self.umax = 5\n",
    "        ts_matrix = pd.read_pickle(ts_matrix_file)\n",
    "        self.ts_keys = ts_matrix[['this_file_name','sentence_order','word_count']]\n",
    "        ts_matrix = ts_matrix.drop(columns = ['this_file_name','sentence_order','word_count'])\n",
    "        # we must then build the ts-isf matrix:\n",
    "        # we start by building the isf vector\n",
    "        isf = np.log(float(ts_matrix.shape[0])/(ts_matrix >0).sum(axis = 0))\n",
    "\n",
    "        # we then build the ts_isf matrix:\n",
    "        ts_isf = (ts_matrix*isf).fillna(0).as_matrix()\n",
    "        self.ts_isf = ts_isf\n",
    "        # we then define the similarity between two vectors as the cosine similarity\n",
    "        self.sentence_similarities  = cosine_similarity(ts_isf.mean(axis = 0).reshape(1,-1),ts_isf)\n",
    "        self.full_similarities = cosine_similarity(ts_isf,ts_isf)\n",
    "        self.mean_overall_vector = self.ts_isf.mean(axis = 0)\n",
    "    def get_coverage(self,X):\n",
    "        if(X.sum() == 0):\n",
    "            return 0\n",
    "        else:\n",
    "            summary = self.ts_isf[X,:]\n",
    "            mean_summary_vector = summary.mean(axis = 0)\n",
    "            coverage = cosine_similarity(self.mean_overall_vector.reshape(1,-1),mean_summary_vector.reshape(1,-1))*np.matmul(self.sentence_similarities,X)\n",
    "        return coverage[0][0]\n",
    "\n",
    "    def get_diversity(self,X):\n",
    "        if(X.sum() == 0):\n",
    "            return 1000000\n",
    "        # we start by creating a map of which sentences are to be selected in this measure\n",
    "        selection_matrix = np.matmul(X.reshape(-1,1),X.reshape(1,-1))\n",
    "        diversity  = float((np.multiply(self.full_similarities,selection_matrix)).sum() - X.sum())/2\n",
    "        if(diversity == 0):\n",
    "            return 10000\n",
    "        return diversity\n",
    "\n",
    "    def objective_function(self,X):\n",
    "        a = self.get_coverage(X)\n",
    "        b = self.get_diversity(X)\n",
    "        return(float(a)/(b))\n",
    "\n",
    "    def get_FT(self,iterations,itermax):\n",
    "        return np.exp(-2*float(iterations)/self.itermax)\n",
    "\n",
    "    def clip_population(self,population):\n",
    "        umin = self.umin\n",
    "        umax = self.umax\n",
    "        population[population < umin] = 2*umin - population[population < umin]\n",
    "        population[population > umax] = 2*umax - population[population > umax]\n",
    "        return population\n",
    "\n",
    "    def get_crossover_rate(self,actual_scores):\n",
    "        RD = (self.max_score - actual_scores)/float((self.max_score - self.min_score))\n",
    "        hiptan = np.tanh(2*RD)\n",
    "        CR = 2*hiptan/(1+hiptan)\n",
    "        return CR\n",
    "\n",
    "    def binarize(self,population):\n",
    "        random_draw = np.random.rand(population.shape[0],population.shape[1])\n",
    "        new_X = (random_draw < expit(population.values)).astype(int)\n",
    "        return new_X\n",
    "\n",
    "    def evaluate_summary_candidates(self,X):\n",
    "        scores = []\n",
    "        for i in range(X.shape[0]): \n",
    "            scores.append(self.objective_function(X[i,:]))\n",
    "        return pd.Series(scores)\n",
    "\n",
    "    def perform_crossover(self,population):\n",
    "        reference = population.sample(frac = 1, replace = True).reset_index(drop = True)\n",
    "        FT = self.get_FT(self.iteration,self.itermax)\n",
    "        mutated_candidates = population + (1.0-FT)*(self.global_best_individual - reference) + FT*(self.best_individual - reference)\n",
    "        mutated_candidates = self.clip_population(mutated_candidates)\n",
    "        CR = self.get_crossover_rate(self.scores)\n",
    "        CR = np.repeat(CR.values,population.shape[1])\n",
    "        CR = CR.reshape((self.P,-1))\n",
    "        random_draw = pd.DataFrame(np.random.rand(population.shape[0],population.shape[1]))\n",
    "        keepers = (random_draw <= CR).values\n",
    "        K = np.floor(float(population.shape[1])*np.random.rand(population.shape[0])).astype(int)\n",
    "        future_generation = population.copy()\n",
    "        future_generation[keepers] = mutated_candidates[keepers]\n",
    "        for i in future_generation.index.values:\n",
    "            future_generation.iloc[i,K[i]] = mutated_candidates.loc[i,K[i]]\n",
    "        future_generation = self.clip_population(future_generation)\n",
    "        return future_generation\n",
    "    \n",
    "    def summarize(self):\n",
    "        sentence_limit = self.sentence_limit\n",
    "        P = self.P \n",
    "        itermax = self.itermax\n",
    "        sentence_limit = self.sentence_limit \n",
    "        umin = self.umin\n",
    "        umax = self.umax\n",
    "        ts_keys = self.ts_keys\n",
    "        ts_isf = self.ts_isf\n",
    "        self.iteration = 0\n",
    "        initial_population = pd.DataFrame(10.0*(np.random.rand(P,ts_isf.shape[0]))-(1-sentence_limit/self.ts_keys.word_count.sum())*10)\n",
    "        initial_population = self.clip_population(initial_population)\n",
    "        # we then discretize the problem\n",
    "        X = self.binarize(initial_population)\n",
    "\n",
    "        # we check which of the sentences is possibly valid by checking the word count of the summary\n",
    "        invalid_score = (sentence_limit - np.matmul(X,self.ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,))\n",
    "        invalid_score = invalid_score/sentence_limit\n",
    "        invalid_population = (invalid_score < 0)\n",
    "\n",
    "        # we then compute the population scores\n",
    "        self.scores = self.evaluate_summary_candidates(X)\n",
    "        self.scores[invalid_population] = invalid_score[invalid_population]\n",
    "        # find the best and worst individuals\n",
    "        scores = self.scores\n",
    "        self.max_score = scores.max()\n",
    "        self.min_score = scores.min()\n",
    "        self.best_individual = initial_population.loc[scores[scores.values == self.scores.max()].index[0],:].values\n",
    "        self.worst_individual = initial_population.loc[scores[scores.values == self.scores.min()].index[0],:].values\n",
    "        # at first, the global best and the best are the same\n",
    "        self.global_best_individual = self.best_individual\n",
    "        self.global_best_score = self.max_score\n",
    "        next_generation = self.perform_crossover(initial_population)\n",
    "        X_mutated = self.binarize(next_generation)\n",
    "        # we then compute if the mutated scores are valid or not\n",
    "        invalid_score = sentence_limit - np.matmul(X_mutated,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,)\n",
    "        invalid_score = invalid_score/sentence_limit\n",
    "        invalid_population = (invalid_score < 0)\n",
    "        # we then compute the mutated scores\n",
    "        mutated_scores = self.evaluate_summary_candidates(X_mutated)\n",
    "        mutated_scores[invalid_population] = invalid_score\n",
    "        if(mutated_scores.max() > self.max_score):\n",
    "            max_score = mutated_scores.max()\n",
    "            self.max_score = max_score\n",
    "            self.max_score = mutated_scores.max()\n",
    "            self.best_individual = next_generation.iloc[mutated_scores[mutated_scores == max_score].index[0],:].values\n",
    "            if(self.max_score > self.global_best_score):\n",
    "                self.global_best_score = self.max_score\n",
    "                self.global_best_individual = next_generation.loc[mutated_scores[mutated_scores == self.max_score].index[0],:].values\n",
    "        worse = (mutated_scores < scores).values\n",
    "\n",
    "        next_generation.loc[worse,:] = initial_population.loc[worse,:] \n",
    "        X_mutated[worse,:] = X[worse,:]\n",
    "        X = X_mutated\n",
    "        worse.sum()\n",
    "        # we then iterate:\n",
    "        for iteration in range(1,itermax):\n",
    "            # we then repeat the process for each successive iteration:\n",
    "            self.iteration = iteration\n",
    "            initial_population = next_generation.copy()\n",
    "            X = self.binarize(initial_population)\n",
    "            # we then discretize the problem\n",
    "\n",
    "            # we check which of the sentences is possibly valid by checking the word count of the summary\n",
    "            invalid_score = sentence_limit - np.matmul(X,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,)\n",
    "            invalid_score = invalid_score/sentence_limit\n",
    "            invalid_population = (invalid_score < 0)\n",
    "            #we then binarize the population:\n",
    "\n",
    "            # we then compute the population scores\n",
    "            self.scores = self.evaluate_summary_candidates(X)\n",
    "            scores = self.scores\n",
    "            self.scores[invalid_population] = invalid_score[invalid_population]\n",
    "            # find the best and worst individuals\n",
    "            self.max_score = scores.max()\n",
    "            self.min_score = scores.min()\n",
    "            self.best_individual = initial_population.loc[scores[scores.values == self.scores.max()].index[0],:].values\n",
    "\n",
    "            if(self.max_score > self.global_best_score):\n",
    "                self.global_best_score = self.max_score\n",
    "                self.global_best_individual = self.best_individual\n",
    "                self.global_best_X = X[scores[scores.values == scores.max()].index[0]]\n",
    "            next_generation = self.perform_crossover(initial_population)\n",
    "            X_mutated = self.binarize(next_generation)\n",
    "            # we then compute if the mutated scores are valid or not\n",
    "            invalid_score = sentence_limit - np.matmul(X_mutated,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,)\n",
    "            invalid_score = invalid_score/sentence_limit\n",
    "            invalid_population = (invalid_score < 0)\n",
    "            # we then compute the mutated scores\n",
    "            mutated_scores = self.evaluate_summary_candidates(X_mutated)\n",
    "            mutated_scores[invalid_population] = invalid_score\n",
    "            if(mutated_scores.max() > self.max_score):\n",
    "                self.max_score = mutated_scores.max()\n",
    "                self.best_individual = next_generation.iloc[mutated_scores[mutated_scores == self.max_score].index[0],:].values\n",
    "                if(self.max_score > self.global_best_score):\n",
    "                    self.global_best_score = self.max_score\n",
    "                    self.global_best_individual = next_generation.iloc[mutated_scores[mutated_scores == self.max_score].index[0],:].values\n",
    "                    self.global_best_X = X_mutated[mutated_scores[mutated_scores.values == mutated_scores.max()].index[0],:]\n",
    "            worse = mutated_scores < scores\n",
    "            next_generation.loc[worse,:] = initial_population.loc[worse,:]\n",
    "            X_mutated[worse,:] = X[worse,:]\n",
    "            X = X_mutated\n",
    "            if(iteration%25 == 0):\n",
    "                print(iteration,mutated_scores.mean(),scores.mean(),scores.max(),worse.sum(),self.global_best_score)\n",
    "        selected_sentences = self.sentence_bank.loc[self.global_best_X == 1, :].sentence\n",
    "        abstract= ''\n",
    "        for i in selected_sentences:\n",
    "            abstract += i\n",
    "        \n",
    "        return abstract,self.ts_matrix_file,self.global_best_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(ts_matrix_file):\n",
    "    my_summarizer = summarizer(ts_matrix_file)\n",
    "    result = my_summarizer.summarize()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs = 8, verbose = 11)(delayed(compute_results)(ts_matrix_file)for ts_matrix_file in ts_matrix_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Time : 218 minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_summaries = pd.Series(sorted(glob.glob('/home/joao/Thesis/simplified_abstracts/*')))\n",
    "true_summaries[0]\n",
    "\n",
    "total_scores = []\n",
    "i = result[1]\n",
    "this_file = i.split('/')[-1]\n",
    "this_file_num = this_file.split('.')[0][1:4]\n",
    "ground_truths = true_summaries[true_summaries.str[-7:-4] == this_file_num]\n",
    "scores = []\n",
    "rouge = Rouge()\n",
    "abstract = result[0]\n",
    "for j in ground_truths:\n",
    "    with open(j,'rb') as f:\n",
    "        ground_truth = f.read()\n",
    "        tmp_scores = rouge.get_scores(abstract,ground_truth,avg = True)\n",
    "    scores.append(tmp_scores['rouge-2']['p'])\n",
    "total_scores.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_summarizer.get_coverage(result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "files = []\n",
    "for i in results:\n",
    "    abstracts.append(i[0])\n",
    "    files.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame({'abstracts':abstracts,'files':files})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_pickle('/home/joao/Thesis/OCDSumSaDE/final_results.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now running it for the test_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit\n",
    "from rouge import Rouge,FilesRouge\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# We start by importing an example of a term_sentence matrix\n",
    "ts_matrix_files = sorted(glob.glob('/home/joao/Thesis/test_set/ts_matrices/*.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class summarizer:\n",
    "    def __init__(self,ts_matrix_file):\n",
    "        self.ts_matrix_file = ts_matrix_file\n",
    "        sentence_bank_file = '/home/joao/Thesis/test_set/sentence_banks/' + ts_matrix_file.split('/')[-1]\n",
    "        self.sentence_bank = pd.read_pickle(sentence_bank_file)\n",
    "        self.P = 100\n",
    "        self.itermax = 1000\n",
    "        self.sentence_limit = 250.0\n",
    "        self.umin = -5\n",
    "        self.umax = 5\n",
    "        ts_matrix = pd.read_pickle(ts_matrix_file)\n",
    "        self.ts_keys = ts_matrix[['this_file_name','sentence_order','word_count']]\n",
    "        ts_matrix = ts_matrix.drop(columns = ['this_file_name','sentence_order','word_count'])\n",
    "        # we must then build the ts-isf matrix:\n",
    "        # we start by building the isf vector\n",
    "        isf = np.log(float(ts_matrix.shape[0])/(ts_matrix >0).sum(axis = 0))\n",
    "\n",
    "        # we then build the ts_isf matrix:\n",
    "        ts_isf = (ts_matrix*isf).fillna(0).as_matrix()\n",
    "        self.ts_isf = ts_isf\n",
    "        # we then define the similarity between two vectors as the cosine similarity\n",
    "        self.sentence_similarities  = cosine_similarity(ts_isf.mean(axis = 0).reshape(1,-1),ts_isf)\n",
    "        self.full_similarities = cosine_similarity(ts_isf,ts_isf)\n",
    "        self.mean_overall_vector = self.ts_isf.mean(axis = 0)\n",
    "    def get_coverage(self,X):\n",
    "        if(X.sum() == 0):\n",
    "            return 0\n",
    "        else:\n",
    "            summary = self.ts_isf[X,:]\n",
    "            mean_summary_vector = summary.mean(axis = 0)\n",
    "            coverage = cosine_similarity(self.mean_overall_vector.reshape(1,-1),mean_summary_vector.reshape(1,-1))*np.matmul(self.sentence_similarities,X)\n",
    "        return coverage[0][0]\n",
    "\n",
    "    def get_diversity(self,X):\n",
    "        if(X.sum() == 0):\n",
    "            return 1000000\n",
    "        # we start by creating a map of which sentences are to be selected in this measure\n",
    "        selection_matrix = np.matmul(X.reshape(-1,1),X.reshape(1,-1))\n",
    "        diversity  = float((np.multiply(self.full_similarities,selection_matrix)).sum() - X.sum())/2\n",
    "        if(diversity == 0):\n",
    "            return 10000\n",
    "        return diversity\n",
    "\n",
    "    def objective_function(self,X):\n",
    "        a = self.get_coverage(X)\n",
    "        b = self.get_diversity(X)\n",
    "        return(float(a)/max(b,0.1))\n",
    "\n",
    "    def get_FT(self,iterations,itermax):\n",
    "        return np.exp(-2*float(iterations)/self.itermax)\n",
    "\n",
    "    def clip_population(self,population):\n",
    "        umin = self.umin\n",
    "        umax = self.umax\n",
    "        population[population < umin] = 2*umin - population[population < umin]\n",
    "        population[population > umax] = 2*umax - population[population > umax]\n",
    "        return population\n",
    "\n",
    "    def get_crossover_rate(self,actual_scores):\n",
    "        RD = (self.max_score - actual_scores)/float((self.max_score - self.min_score))\n",
    "        hiptan = np.tanh(2*RD)\n",
    "        CR = 2*hiptan/(1+hiptan)\n",
    "        return CR\n",
    "\n",
    "    def binarize(self,population):\n",
    "        random_draw = np.random.rand(population.shape[0],population.shape[1])\n",
    "        new_X = (random_draw < expit(population.values)).astype(int)\n",
    "        return new_X\n",
    "\n",
    "    def evaluate_summary_candidates(self,X):\n",
    "        scores = []\n",
    "        for i in range(X.shape[0]): \n",
    "            scores.append(self.objective_function(X[i,:]))\n",
    "        return pd.Series(scores)\n",
    "\n",
    "    def perform_crossover(self,population):\n",
    "        reference = population.sample(frac = 1, replace = True).reset_index(drop = True)\n",
    "        FT = self.get_FT(self.iteration,self.itermax)\n",
    "        mutated_candidates = population + (1.0-FT)*(self.global_best_individual - reference) + FT*(self.best_individual - reference)\n",
    "        mutated_candidates = self.clip_population(mutated_candidates)\n",
    "        CR = self.get_crossover_rate(self.scores)\n",
    "        CR = np.repeat(CR.values,population.shape[1])\n",
    "        CR = CR.reshape((self.P,-1))\n",
    "        random_draw = pd.DataFrame(np.random.rand(population.shape[0],population.shape[1]))\n",
    "        keepers = (random_draw <= CR).values\n",
    "        K = np.floor(float(population.shape[1])*np.random.rand(population.shape[0])).astype(int)\n",
    "        future_generation = population.copy()\n",
    "        future_generation[keepers] = mutated_candidates[keepers]\n",
    "        for i in future_generation.index.values:\n",
    "            future_generation.iloc[i,K[i]] = mutated_candidates.loc[i,K[i]]\n",
    "        future_generation = self.clip_population(future_generation)\n",
    "\n",
    "        return future_generation\n",
    "    \n",
    "    def summarize(self):\n",
    "        sentence_limit = self.sentence_limit\n",
    "        P = self.P \n",
    "        itermax = self.itermax\n",
    "        sentence_limit = self.sentence_limit \n",
    "        umin = self.umin\n",
    "        umax = self.umax\n",
    "        ts_keys = self.ts_keys\n",
    "        ts_isf = self.ts_isf\n",
    "        self.iteration = 0\n",
    "        initial_population = pd.DataFrame(10.0*(np.random.rand(P,ts_isf.shape[0]))-(1-sentence_limit/self.ts_keys.word_count.sum())*10)\n",
    "        initial_population = self.clip_population(initial_population)\n",
    "        # we then discretize the problem\n",
    "        X = self.binarize(initial_population)\n",
    "        # we check which of the sentences is possibly valid by checking the word count of the summary\n",
    "        invalid_score = (sentence_limit - np.matmul(X,self.ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,))\n",
    "        invalid_score = invalid_score/sentence_limit\n",
    "        invalid_population = (invalid_score < 0)\n",
    "\n",
    "        # we then compute the population scores\n",
    "        self.scores = self.evaluate_summary_candidates(X)\n",
    "        self.scores[invalid_population] = invalid_score[invalid_population]\n",
    "        # find the best and worst individuals\n",
    "        scores = self.scores\n",
    "        self.max_score = scores.max()\n",
    "        self.min_score = scores.min()\n",
    "        self.best_individual = initial_population.loc[scores[scores.values == self.scores.max()].index[0],:].values\n",
    "        self.worst_individual = initial_population.loc[scores[scores.values == self.scores.min()].index[0],:].values\n",
    "        # at first, the global best and the best are the same\n",
    "        self.global_best_individual = self.best_individual\n",
    "        self.global_best_score = self.max_score\n",
    "        next_generation = self.perform_crossover(initial_population)\n",
    "        X_mutated = self.binarize(next_generation)\n",
    "        # we then compute if the mutated scores are valid or not\n",
    "        invalid_score = sentence_limit - np.matmul(X_mutated,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,)\n",
    "        invalid_score = invalid_score/sentence_limit\n",
    "        invalid_population = (invalid_score < 0)\n",
    "        # we then compute the mutated scores\n",
    "        mutated_scores = self.evaluate_summary_candidates(X_mutated)\n",
    "        mutated_scores[invalid_population] = invalid_score\n",
    "        if(mutated_scores.max() > self.max_score):\n",
    "            max_score = mutated_scores.max()\n",
    "            self.max_score = max_score\n",
    "            self.max_score = mutated_scores.max()\n",
    "            self.best_individual = next_generation.iloc[mutated_scores[mutated_scores == max_score].index[0],:].values\n",
    "            if(self.max_score > self.global_best_score):\n",
    "                self.global_best_score = self.max_score\n",
    "                self.global_best_individual = next_generation.loc[mutated_scores[mutated_scores == self.max_score].index[0],:].values\n",
    "        worse = (mutated_scores < scores).values\n",
    "\n",
    "        next_generation.loc[worse,:] = initial_population.loc[worse,:] \n",
    "        X_mutated[worse,:] = X[worse,:]\n",
    "        X = X_mutated\n",
    "        worse.sum()\n",
    "        # we then iterate:\n",
    "        for iteration in range(1,itermax):\n",
    "            # we then repeat the process for each successive iteration:\n",
    "            self.iteration = iteration\n",
    "            initial_population = next_generation.copy()\n",
    "            X = self.binarize(initial_population)\n",
    "            # we then discretize the problem\n",
    "\n",
    "            # we check which of the sentences is possibly valid by checking the word count of the summary\n",
    "            invalid_score = sentence_limit - np.matmul(X,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,)\n",
    "            invalid_score = invalid_score/sentence_limit\n",
    "            invalid_population = (invalid_score < 0)\n",
    "            #we then binarize the population:\n",
    "\n",
    "            # we then compute the population scores\n",
    "            self.scores = self.evaluate_summary_candidates(X)\n",
    "            scores = self.scores\n",
    "            self.scores[invalid_population] = invalid_score[invalid_population]\n",
    "                # find the best and worst individuals\n",
    "            self.max_score = scores.max()\n",
    "            self.min_score = scores.min()\n",
    "            self.best_individual = initial_population.loc[scores[scores.values == self.scores.max()].index[0],:].values\n",
    "\n",
    "            if(self.max_score > self.global_best_score):\n",
    "                self.global_best_score = self.max_score\n",
    "                self.global_best_individual = self.best_individual\n",
    "                self.global_best_X = X[scores[scores.values == scores.max()].index[0]]\n",
    "            next_generation = self.perform_crossover(initial_population)\n",
    "            X_mutated = self.binarize(next_generation)\n",
    "            # we then compute if the mutated scores are valid or not\n",
    "            invalid_score = sentence_limit - np.matmul(X_mutated,ts_keys['word_count'].values.reshape(-1,1)).reshape(-1,)\n",
    "            invalid_score = invalid_score/sentence_limit\n",
    "            invalid_population = (invalid_score < 0)\n",
    "            # we then compute the mutated scores\n",
    "            mutated_scores = self.evaluate_summary_candidates(X_mutated)\n",
    "            mutated_scores[invalid_population] = invalid_score\n",
    "            if(mutated_scores.max() > self.max_score):\n",
    "                self.max_score = mutated_scores.max()\n",
    "                self.best_individual = next_generation.iloc[mutated_scores[mutated_scores == self.max_score].index[0],:].values\n",
    "                if(self.max_score > self.global_best_score):\n",
    "                    self.global_best_score = self.max_score\n",
    "                    self.global_best_individual = next_generation.iloc[mutated_scores[mutated_scores == self.max_score].index[0],:].values\n",
    "                    self.global_best_X = X_mutated[mutated_scores[mutated_scores.values == mutated_scores.max()].index[0],:]\n",
    "            worse = mutated_scores < scores\n",
    "            next_generation.loc[worse,:] = initial_population.loc[worse,:]\n",
    "            X_mutated[worse,:] = X[worse,:]\n",
    "            X = X_mutated\n",
    "            if(iteration%25 == 0):\n",
    "                print(iteration,mutated_scores.mean(),scores.mean(),scores.max(),worse.sum(),self.global_best_score)\n",
    "        selected_sentences = self.sentence_bank.loc[self.global_best_X == 1, :].sentence\n",
    "        abstract= ''\n",
    "        for i in selected_sentences:\n",
    "            abstract += i\n",
    "        \n",
    "        return abstract,self.ts_matrix_file,self.global_best_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(ts_matrix_file):\n",
    "    my_summarizer = summarizer(ts_matrix_file)\n",
    "    result = my_summarizer.summarize()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs = 8, verbose = 11)(delayed(compute_results)(ts_matrix_file)for ts_matrix_file in ts_matrix_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "files = []\n",
    "for i in results:\n",
    "    abstracts.append(i[0])\n",
    "    files.append(i[1])\n",
    "\n",
    "final_results = pd.DataFrame({'abstracts':abstracts,'files':files})\n",
    "\n",
    "final_results.to_pickle('/home/joao/Thesis/OCDSumSaDE/test_final_results.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution time: 51.2min finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge,FilesRouge\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "results_df = pd.read_pickle('/home/joao/Thesis/OCDSumSaDE/test_final_results.p')\n",
    "\n",
    "results_df.files = results_df.files.str.split('/', expand = True)[6]\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"This function removes all non-ascii characters from text and replaces them with their closest ascii representation\"\"\"\n",
    "    return unidecode(unicode(text, encoding = \"utf-8\"))\n",
    "# we then load all summaries and candidate summaries:\n",
    "\n",
    "total_scores = []\n",
    "scores = []\n",
    "r1 = []\n",
    "r2 = []\n",
    "rl = []\n",
    "for i in tqdm(results_df.index):\n",
    "    ground_truth = '/home/joao/Thesis/test_set/abstracts/ground_truths/'+ results_df.loc[i,'files'][:-2]+'.txt'\n",
    "    rouge = Rouge()\n",
    "    with open(ground_truth,'rb') as f:\n",
    "        ground_truth = f.read()\n",
    "    ground_truth = remove_non_ascii(ground_truth)\n",
    "    tmp_scores = rouge.get_scores(results_df.loc[i,'abstracts'],ground_truth, avg = True)\n",
    "    r2.append(tmp_scores['rouge-2']['f'])\n",
    "    r1.append(tmp_scores['rouge-1']['f'])\n",
    "    rl.append(tmp_scores['rouge-l']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('r1',np.mean(r1),np.std(r1,ddof = 1))\n",
    "print('r2',np.mean(r2),np.std(r2, ddof = 1))\n",
    "print('rl',np.mean(rl),np.std(rl, ddof = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('r1', 0.29292168257691364, 0.058581228141602736)\n",
    "\n",
    "('r2', 0.07615283246687429, 0.04955519211966711)\n",
    "\n",
    "('rl', 0.24793091807760487, 0.05986021707431101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
